
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine learning in bioinformatics &#8212; An Introduction to Applied Bioinformatics</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Sequence homology searching" href="database-searching.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">An Introduction to Applied Bioinformatics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="biological-information.html">
   Biological Information
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pairwise-alignment.html">
   Pairwise sequence alignment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="database-searching.html">
   Sequence homology searching
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Machine learning in bioinformatics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/machine-learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/machine-learning.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/applied-bioinformatics/iab2/main?urlpath=tree/book/machine-learning.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-feature-table">
   The feature table
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-iris-dataset">
     The Iris dataset
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unsupervised-versus-supervised-learning-methods">
   Unsupervised versus supervised learning methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-methods-applied-to-microbial-sequence-data">
   Machine learning methods applied to microbial sequence data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unsupervised-learning">
   Unsupervised learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-distances-between-samples">
     Computing distances between samples
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polar-ordination">
     Polar ordination
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-ordination-plots">
     Interpreting ordination plots
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#axis-order">
       Axis order
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#uncorrelated-axes">
       Uncorrelated axes
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#directionality-of-the-axes">
       Directionality of the axes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principle-coordinates-analysis-pcoa">
     Principle Coordinates Analysis (PCoA)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-classification">
   Supervised classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-a-classification-task">
     Defining a classification task
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-data-test-data-and-cross-validation">
     Training data, test data, and cross-validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluating-a-binary-classifier">
     Evaluating a binary classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#naive-bayes-classifiers">
     Naive Bayes classifiers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-a-native-bayes-classifier">
     Training a Native Bayes classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-a-naive-bayes-classifier">
     Applying a Naive Bayes classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluating-our-confidence-in-the-results-of-the-naive-bayes-classifier">
     Evaluating our confidence in the results of the Naive Bayes classifier
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variations-on-the-input-to-machine-learning-algorithms">
   Variations on the input to machine learning algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#list-of-works-cited">
   List of works cited
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="machine-learning-in-bioinformatics">
<h1>Machine learning in bioinformatics<a class="headerlink" href="#machine-learning-in-bioinformatics" title="Permalink to this headline">¶</a></h1>
<p>In this chapter we’ll begin talking about machine learning algorithms. Machine learning algorithms are used in bioinformatics for tasks where the user would like an algorithm to assist in the identification of patterns in a complex dataset. As is typically the case in this book, we’ll work through implementing a few algorithms but these are not the implementations that you should use in practice. The code is written to be accessible for learning. <a class="reference external" href="http://scikit-learn.org/">scikit-learn</a> is a popular and well-documented Python library for machine learning which many bioinformatics researchers and software developers use in their work. If you’d like to start trying some of these tools out, scikit-learn is a great place to start.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Machine learning algorithms can easily be misused, either intentionally or unintentionally, to provide misleading results. This chapter will cover some guidelines for how to use these techniques, but it is only intended as a primer to introduce machine learning. It’s not a detailed discussion of how machine learning algorithms should and shouldn’t be used. If you want to start applying machine learning tools in your own research, I recommend moving from this chapter to the scikit-learn documentation, and their content on <a class="reference external" href="https://scikit-learn.org/stable/common_pitfalls.html">Common pitfalls and recommended practices</a>.</p>
</div>
<div class="section" id="the-feature-table">
<h2>The feature table<a class="headerlink" href="#the-feature-table" title="Permalink to this headline">¶</a></h2>
<p>Machine learning algorithms generally are provided with a table of <strong>samples</strong> and user-defined <strong>features</strong> of those samples. These data are typically represented in a matrix, where samples are the rows and features are the columns. This matrix is referred to as a <strong>feature table</strong>, and it is central to machine learning and many subfields of bioinformatics. The terms used here are purposefully general. Samples are intended to be any unit of study, and features are attributes of those samples. Sometimes <strong>labels</strong> or <strong>response variables</strong> will also be associated with the samples, in which case a different class of methods can be applied.</p>
<p>scikit-learn provides a few example datasets that can be used for learning. Let’s start by taking a look and one of them to get an idea of what input might look like in a machine learning task.</p>
<div class="section" id="the-iris-dataset">
<h3>The Iris dataset<a class="headerlink" href="#the-iris-dataset" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset">Iris dataset</a> is a classic example used in machine learning, originally published by RA Fisher <span id="id1">[<a class="reference internal" href="#id28">Fis36</a>]</span>. This feature table describes four features of 150 specimens of Iris, a genus of flowering plant, representing three species. The feature table follows:</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell loads data from scikit-learn and organizes it into some strcutures that</span>
<span class="c1"># we&#39;ll use to conveniently view the data.</span>

<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">iris_dataset</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">iris_feature_table</span> <span class="o">=</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">iris_feature_table</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;sample-id&#39;</span>
<span class="c1"># map target integers onto species names</span>
<span class="n">iris_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="p">],</span> 
                        <span class="n">index</span><span class="o">=</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;species&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">iris_labels</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;sample-id&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris_feature_table</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
    <tr>
      <th>sample-id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 4 columns</p>
</div></div></div>
</div>
<p>The rows in this table represent our samples - in this case specimens of Iris. The columns represent features, or attributes of the samples. Each <strong>sample vector</strong> (i.e., row) will include a unique identifier for the sample which we usually call the <em>sample id</em> (here these are simply integers), and values for each feature for that sample. Each <strong>feature vector</strong> (i.e., column) will similarly contain an identifier for the feature, or the the <em>feature id</em>. These are often simplistic descriptions of the features, as they are in this example, but they don’t need to be (integers would work fine as feature ids). The feature vector then contains the values measured for that feature in each sample.</p>
<p>This feature table on its own can serve as an input dataset for unsupervised learning tasks, which we’ll cover first in this chapter. A goal of unsupervised learning might be to determine if there are groups of samples that are most similar to one another.</p>
<p>In addition to this feature table, the Iris dataset contains labels for each of the 150 samples indicating which species each sample belongs to:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris_labels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
    </tr>
    <tr>
      <th>sample-id</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>setosa</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>virginica</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 1 columns</p>
</div></div></div>
</div>
<p>The sample ids in this label vector must be the same as the sample ids in the feature table. The feature table and the sample labels together can be used as input data for supervised learning tasks, which we’ll cover second in this chapter. A goal of supervised learning might be to develop a classifier that could report the species of an Iris if provided with values for its sepal length and width and its petal length and width (i.e., the features that the algorithm originally had access).</p>
<p>There are three different labels, or classes, in this dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris_labels</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="unsupervised-versus-supervised-learning-methods">
<h2>Unsupervised versus supervised learning methods<a class="headerlink" href="#unsupervised-versus-supervised-learning-methods" title="Permalink to this headline">¶</a></h2>
<p>Many machine learning methods are classified at a high level as either unsupervised or supervised learning methods.</p>
<p>In <strong>unsupervised learning</strong> we either don’t have or don’t use sample labels, and the algorithm therefore operates on a feature table alone. Typically the user is hoping to discover some structure in the data that can help them to understand which samples are most similar to each other based on their feature values. In this chapter we’ll introduce ordination as an unsupervised learning task. Ordination is very widely used in biology - you may have already encountered ordination plots (such as PCoA or NMDS plots) in some of your own work.</p>
<p>In <strong>supervised learning</strong>, on the other hand, sample labels are used in addition to a feature table. The sample labels can be discrete, as in the Iris dataset, or continuous, and that distinction defines whether we’re working on a classification or regression task, respectively. The goal of a supervised learning task is typically to have the computer develop a model that can accurate predict an unlabeled sample’s label from its feature values (for example, what species does an Iris specimen belong to, based on its sepal and petal length and width).</p>
</div>
<div class="section" id="machine-learning-methods-applied-to-microbial-sequence-data">
<h2>Machine learning methods applied to microbial sequence data<a class="headerlink" href="#machine-learning-methods-applied-to-microbial-sequence-data" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell performs some configuration for this notebook. It&#39;s hidden by</span>
<span class="c1"># default because it&#39;s not relevant to the content of this chapter. You&#39;ll</span>
<span class="c1"># occasionally notice that I hide this type of information so it&#39;s not </span>
<span class="c1"># distracting.</span>

<span class="o">%</span><span class="k">pylab</span> inline

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">skbio</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">random</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Populating the interactive namespace from numpy and matplotlib
</pre></div>
</div>
</div>
</div>
<p>In this chapter, we’ll work with 16S rRNA data <a class="reference internal" href="database-searching.html#load-qdr"><span class="std std-ref">as we did previously</span></a>. Specifically, we’ll load sequences from the Greengenes database and construct a feature table from them. We’ll use this feature table in an unsupervised learning task and a supervised learning task. We’ll also load labels for the sequences which we’ll primarily use in our supervised learning task, but which we’ll also use to aid in interpretation in our unsupervised learning task.</p>
<p>Our goal with these tasks will be to explore species-level taxonomy of a few microbial species based on sequence data. In our unsupervised learning task, we’ll determine if samples (i.e., sequences) coming from the same species appear to generally be more similar to each other than samples coming from different species. In our supervised learning task, we’ll determine if we can develop a classifier to predict microbial species from an unlabeled sequence.</p>
<p>Let’s start by loading five sequences from each of five specific microbial species from Greengenes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">qiime_default_reference</span> <span class="k">as</span> <span class="nn">qdr</span>
<span class="kn">import</span> <span class="nn">skbio</span>

<span class="k">def</span> <span class="nf">load_annotated_sequences</span><span class="p">(</span><span class="n">taxa_of_interest</span><span class="p">,</span> <span class="n">class_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sequence_length</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> 
                             <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ids_to_exclude</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    
    <span class="c1"># Load the taxonomic data</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">SequenceRecord</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="n">typename</span><span class="o">=</span><span class="s1">&#39;SequenceRecord&#39;</span><span class="p">,</span>
                                            <span class="n">field_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;identifier&#39;</span><span class="p">,</span> <span class="s1">&#39;split_taxonomy&#39;</span><span class="p">,</span> <span class="s1">&#39;taxonomy&#39;</span><span class="p">,</span> <span class="s1">&#39;sequence&#39;</span><span class="p">])</span>
    
    <span class="n">taxon_to_sequence_records</span> <span class="o">=</span> <span class="p">{</span><span class="n">t</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">taxa_of_interest</span><span class="p">}</span>        
    
    <span class="n">id_to_taxonomy_record</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">qdr</span><span class="o">.</span><span class="n">get_reference_taxonomy</span><span class="p">()):</span>
        <span class="n">identifier</span><span class="p">,</span> <span class="n">taxonomy</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">id_to_taxonomy_record</span><span class="p">[</span><span class="n">identifier</span><span class="p">]</span> <span class="o">=</span> <span class="n">taxonomy</span>
    
    <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">skbio</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">qdr</span><span class="o">.</span><span class="n">get_reference_sequences</span><span class="p">(),</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;fasta&#39;</span><span class="p">,</span> 
                             <span class="n">constructor</span><span class="o">=</span><span class="n">skbio</span><span class="o">.</span><span class="n">DNA</span><span class="p">):</span>
        <span class="n">identifier</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ids_to_exclude</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">identifier</span> <span class="ow">in</span> <span class="n">ids_to_exclude</span><span class="p">:</span>
            <span class="c1"># if this id was tagged to not be included in the result, </span>
            <span class="c1"># move on to the next record</span>
            <span class="k">continue</span>
        
        <span class="n">tax</span> <span class="o">=</span> <span class="n">id_to_taxonomy_record</span><span class="p">[</span><span class="n">identifier</span><span class="p">]</span>
        <span class="n">split_taxonomy</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">tax</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;;&#39;</span><span class="p">)]</span>
        <span class="n">taxonomy</span> <span class="o">=</span> <span class="s1">&#39;;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">split_taxonomy</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">taxonomy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">taxon_to_sequence_records</span><span class="p">:</span>
            <span class="c1"># if this is not one of the taxa that we&#39;re interested in, </span>
            <span class="c1"># move on to the next record. </span>
            <span class="k">continue</span>
        
        <span class="k">if</span> <span class="n">seq</span><span class="o">.</span><span class="n">has_degenerates</span><span class="p">():</span>
            <span class="c1"># for the purpose of this exercise we&#39;ll skip records </span>
            <span class="c1"># that have non-ACGT characters. if degenerate characters</span>
            <span class="c1"># are present, move on to the next record</span>
            <span class="k">continue</span>
            
        <span class="k">if</span> <span class="n">sequence_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="n">seq</span><span class="p">[:</span><span class="n">sequence_length</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="n">seq</span>

        <span class="n">sr</span> <span class="o">=</span> <span class="n">SequenceRecord</span><span class="p">(</span><span class="n">identifier</span><span class="o">=</span><span class="n">identifier</span><span class="p">,</span>
                            <span class="n">split_taxonomy</span><span class="o">=</span><span class="n">split_taxonomy</span><span class="p">,</span>
                            <span class="n">taxonomy</span><span class="o">=</span><span class="n">taxonomy</span><span class="p">,</span>
                            <span class="n">sequence</span><span class="o">=</span><span class="n">sequence</span><span class="p">)</span>
        <span class="n">taxon_to_sequence_records</span><span class="p">[</span><span class="n">taxonomy</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sr</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">taxon</span><span class="p">,</span> <span class="n">srs</span> <span class="ow">in</span> <span class="n">taxon_to_sequence_records</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2"> sequences were identified for taxon </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">srs</span><span class="p">),</span> <span class="n">taxon</span><span class="p">))</span>
    
    <span class="k">if</span> <span class="n">class_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="n">sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">:</span> <span class="n">sr</span> <span class="k">for</span> <span class="n">srs</span> <span class="ow">in</span> <span class="n">taxon_to_sequence_records</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">sr</span> <span class="ow">in</span> <span class="n">srs</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">taxon</span><span class="p">,</span> <span class="n">srs</span> <span class="ow">in</span> <span class="n">taxon_to_sequence_records</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">class_size</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">srs</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Class size (</span><span class="si">%d</span><span class="s2">) too large for taxon </span><span class="si">%s</span><span class="s2">, which has only </span><span class="si">%d</span><span class="s2"> non-degenerate sequences.&quot;</span> <span class="o">%</span> 
                                 <span class="p">(</span><span class="n">class_size</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">srs</span><span class="p">)))</span>
            <span class="n">sampled_sequence_records</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">srs</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">class_size</span><span class="p">)</span>
            <span class="n">result</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">:</span> <span class="n">sr</span> <span class="k">for</span> <span class="n">sr</span> <span class="ow">in</span> <span class="n">sampled_sequence_records</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container" id="ml-define-sequences-per-speciesm">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">taxa_of_interest</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__stercorea&#39;</span><span class="p">,</span>
    <span class="s1">&#39;k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__copri&#39;</span><span class="p">,</span>
    <span class="s1">&#39;k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__melaninogenica&#39;</span><span class="p">,</span>
    <span class="s1">&#39;k__Bacteria;p__Bacteroidetes;c__Flavobacteriia;o__Flavobacteriales;f__Flavobacteriaceae;g__Flavobacterium;s__succinicans&#39;</span><span class="p">,</span>
    <span class="s1">&#39;k__Bacteria;p__Actinobacteria;c__Actinobacteria;o__Actinomycetales;f__Propionibacteriaceae;g__Propionibacterium;s__acnes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__veronii&#39;</span><span class="p">,</span>
    <span class="s1">&#39;k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__viridiflava&#39;</span>
<span class="p">}</span>
<span class="n">sequences_per_taxon</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">seq_data</span> <span class="o">=</span> <span class="n">load_annotated_sequences</span><span class="p">(</span><span class="n">taxa_of_interest</span><span class="p">,</span> <span class="n">class_size</span><span class="o">=</span><span class="n">sequences_per_taxon</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>17 sequences were identified for taxon k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__viridiflava.
127 sequences were identified for taxon k__Bacteria;p__Actinobacteria;c__Actinobacteria;o__Actinomycetales;f__Propionibacteriaceae;g__Propionibacterium;s__acnes.
121 sequences were identified for taxon k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__copri.
24 sequences were identified for taxon k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__veronii.
15 sequences were identified for taxon k__Bacteria;p__Bacteroidetes;c__Flavobacteriia;o__Flavobacteriales;f__Flavobacteriaceae;g__Flavobacterium;s__succinicans.
26 sequences were identified for taxon k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__melaninogenica.
35 sequences were identified for taxon k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__stercorea.
</pre></div>
</div>
</div>
</div>
<p>We can look at a few randomly selected records from the data that was just compiled as follows. For each, we have a unique identifier, the source species for the sequence record, and a 16S rRNA sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">sr</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">seq_data</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sr</span><span class="o">.</span><span class="n">taxonomy</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sr</span><span class="o">.</span><span class="n">sequence</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;🦠&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4432796
k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__viridiflava
AGTCGAGCGGTAGAGAGAAGCTTGCTTCTCTTGAGAGCGGCGGACGGGTGAGTAATGCCTATGAATCTGCCTGGTAGTGGGGGATAACGCTCGGAAACGGACGCTAATACCGCATACGTCCTACGGGAGAAAGCAGGGGACCTTCGGGCCTTGCGCTATCAGATGAGCCTAGGTCGGATTACCTAGTTGGTGAGGTAATGGCTCACCAAGGCGACCATCCGTAACTGGTCTGAGAGGATGATCAGTCACACTGGAACTGACACACGGTCCAAACTCCTACGGGAGGCACCAGTGGGGAATATTGGACAATGGGCGAAAGCCTGATCCAGCCATGCCGCGTGTGTGAACAAGGTCTTCGGATTGTAAAGCACTTTAAGTTGGGAGGAAGGGCAGTAACCTAATACGTTATTGTTTTGACGTTACCGACAGAATAACCACCGGCTAACTCTGTGCCAGCAGCCGCGGTAATACAGAGGGTGCAAGCGTTAATCGGAATTACT
🦠
321743
k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__copri
AGAGTTTGATCCTGGCTCAGGATGAACGCTAGCTACAGGCCTAACACATGCAAGTCGAGGGGCAGCATGATTGAAGCTTGCTTCAATTGATGGCGACCGGCGCACGGGTGAGTAACACGTATCCAACCTTCCGCTGACTCGGGGATAGCCTTTCGAAAGAAAGATTAATACCCGATAGCAAAGGATTTCCGCATGGATATCCTTTTAAAGGATTCTGGTCAGCGATGGGGATGCGTTCCATTAGATAGTTGGCGGGGTAACGGCCCACCAAGTCGACGATGGATAGGGGTTCTGAGAGGAAGGTCCCCCACATTGGAACTGAGACACGGTCCAAACTCCTACGGGAGGCAGCAGTGAGGAATATTGGTCAATGGGCGAGAGCCTGAACCAGCCAAGTAGCGTGAAGGAAGACTGCCCTATGGGTTGTAAACTTCTTTTATACGGGAATAAAGTGAGTCTCGTGAGACTTTTTGCATGTACCTTATGAATAAGGACCGGCT
🦠
514723
k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__copri
AGTGTGATCCTGTCTCAGGATGAACGCTAGCTACAGGCTTAACACATGCAAGTCGAGGGGAAACGATATTGGAAGCTTGCTTCCGATAGGCGTCCACCGGCGCACGGGTGAATAACGCGTATCCAACCTGCCCACCACTTGCGGGATAACCTTCGCCAAAGTAAGACTAATACCCAATGATATCTCTACAAGACATCTGAAACAGATTAAAGATATATCGGTGATGGATGGGGATGCGTCTGATAAGCTTGTTGGCGGGGTAATGGCCCACCAAGGCAACGATCAGTAAGGGTTCTGATAGGAACGTCCCCCACATTGGAACTGACACACGGTCCAAACTCCTACGGGAGGCAGCAGTGAGGAATATTGGTCAATGGGCGAGAGCCTGAACCACCCAAGTAGCGTGCACGAAGACGGCCCTATGGGTGGTAAACTGCCTTCATAAGGGAATAAAGTGAGTCTCGTGAGACTTGCTGCATGTACCTTATGAATAAGGACCG
🦠
</pre></div>
</div>
</div>
</div>
<p>The first thing we need to generate from these data is our feature table, which raises the question of which features we want our machine learning algorithms to work with. In the last chapter, we discussed k-mers are length-k stretches of adjacent characters in a sequence. Those k-mers helped us to identify relevant sequences in our database searching, so they may be useful here as well. We don’t necessarily know how long our k-mers should be (i.e., what value <code class="docutils literal notranslate"><span class="pre">k</span></code> should be set to) however. The longer our kmers, the more likely they are to be specific to certain taxa, which is helpful for machine learning tasks. However, if they get too long it becomes less likely that we’ll observe those kmers in other sequences because the longer a k-mer sequence is, the more likely we are to see variation across closely related organisms. This is a problem for machine learning tasks, because we need to identify features that are shared among related samples.</p>
<p>Let’s set <span class="math notranslate nohighlight">\(k=4\)</span>, and use k-mers as the features that will define our sequence records for the examples in this chapter. I chose this value of <span class="math notranslate nohighlight">\(k\)</span> for our work here based on experimentation with multiple Greengenes subsamples. The features could be based on different values of <span class="math notranslate nohighlight">\(k\)</span>, or other features of sequences that you identify. If you have ideas about other values that you could compute from these sequences, come back here and try it out after you’ve finished reading this chapter.</p>
<div class="cell docutils container" id="ml-define-k">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feature_table_from_sequence_records</span><span class="p">(</span><span class="n">sequence_records</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">kmer_frequencies</span> <span class="o">=</span> <span class="p">{</span><span class="n">id_</span> <span class="p">:</span> <span class="n">sr</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">kmer_frequencies</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">id_</span><span class="p">,</span> <span class="n">sr</span> <span class="ow">in</span> <span class="n">sequence_records</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">kmer_frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">result</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;id&#39;</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<p>After extracting all k-mers from the sequences and putting them in a table where the rows are our sequences (indexed by the unique sequence identifiers), the columns represent unique k-mers (labeled by the k-mer itself), and the values are the number of times each k-mer is observed in each sequence, we end up with our feature table for unsupervised and supervised learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_feature_table</span> <span class="o">=</span> <span class="n">feature_table_from_sequence_records</span><span class="p">(</span><span class="n">seq_data</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">sequence_feature_table</span><span class="p">[:</span><span class="mi">12</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TGCA</th>
      <th>GCAA</th>
      <th>CAAG</th>
      <th>AAGT</th>
      <th>AGTC</th>
      <th>GTCC</th>
      <th>TCCA</th>
      <th>CCAG</th>
      <th>CAGC</th>
      <th>AGCG</th>
      <th>...</th>
      <th>CTCG</th>
      <th>CATC</th>
      <th>TTAT</th>
      <th>ACAT</th>
      <th>CGCC</th>
      <th>ACTA</th>
      <th>TATA</th>
      <th>TGTC</th>
      <th>TCAA</th>
      <th>TCGT</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>272189</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>237364</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>6</td>
      <td>2</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1146480</th>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>89393</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4432796</th>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>...</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>925397</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4403262</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>5</td>
      <td>2</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>165421</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>5</td>
      <td>1</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2028260</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>...</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4226754</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>527604</th>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>...</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>323619</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>...</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>12 rows × 256 columns</p>
</div></div></div>
</div>
<p>As mentioned above, supervised learning tasks also require labels. In this example, the labels will be the species that each sequence was identified in. We’ll next compile our sample label vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feature_labels_from_sequence_records</span><span class="p">(</span><span class="n">sequence_records</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">id_</span><span class="p">:</span><span class="n">sr</span><span class="o">.</span><span class="n">split_taxonomy</span> <span class="k">for</span> <span class="n">id_</span><span class="p">,</span> <span class="n">sr</span> <span class="ow">in</span> <span class="n">sequence_records</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span><span class="o">.</span><span class="n">T</span>
    <span class="n">result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;domain&#39;</span><span class="p">,</span> <span class="s1">&#39;phylum&#39;</span><span class="p">,</span> <span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="s1">&#39;order&#39;</span><span class="p">,</span> <span class="s1">&#39;family&#39;</span><span class="p">,</span> <span class="s1">&#39;genus&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">]</span>
    <span class="n">result</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;id&#39;</span>
    <span class="n">legend_entries</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[[</span><span class="s1">&#39;genus&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">,</span> <span class="s1">&#39;phylum&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">legend_entries</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> (</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">p</span><span class="p">[</span><span class="mi">3</span><span class="p">:]))</span>
    <span class="n">result</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">legend_entries</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_labels</span> <span class="o">=</span> <span class="n">feature_labels_from_sequence_records</span><span class="p">(</span><span class="n">seq_data</span><span class="p">)</span>
<span class="n">sequence_labels</span><span class="p">[:</span><span class="mi">12</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>phylum</th>
      <th>class</th>
      <th>order</th>
      <th>family</th>
      <th>genus</th>
      <th>species</th>
      <th>legend entry</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>272189</th>
      <td>k__Bacteria</td>
      <td>p__Proteobacteria</td>
      <td>c__Gammaproteobacteria</td>
      <td>o__Pseudomonadales</td>
      <td>f__Pseudomonadaceae</td>
      <td>g__Pseudomonas</td>
      <td>s__viridiflava</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
    </tr>
    <tr>
      <th>237364</th>
      <td>k__Bacteria</td>
      <td>p__Proteobacteria</td>
      <td>c__Gammaproteobacteria</td>
      <td>o__Pseudomonadales</td>
      <td>f__Pseudomonadaceae</td>
      <td>g__Pseudomonas</td>
      <td>s__viridiflava</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
    </tr>
    <tr>
      <th>1146480</th>
      <td>k__Bacteria</td>
      <td>p__Proteobacteria</td>
      <td>c__Gammaproteobacteria</td>
      <td>o__Pseudomonadales</td>
      <td>f__Pseudomonadaceae</td>
      <td>g__Pseudomonas</td>
      <td>s__viridiflava</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
    </tr>
    <tr>
      <th>89393</th>
      <td>k__Bacteria</td>
      <td>p__Proteobacteria</td>
      <td>c__Gammaproteobacteria</td>
      <td>o__Pseudomonadales</td>
      <td>f__Pseudomonadaceae</td>
      <td>g__Pseudomonas</td>
      <td>s__viridiflava</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
    </tr>
    <tr>
      <th>4432796</th>
      <td>k__Bacteria</td>
      <td>p__Proteobacteria</td>
      <td>c__Gammaproteobacteria</td>
      <td>o__Pseudomonadales</td>
      <td>f__Pseudomonadaceae</td>
      <td>g__Pseudomonas</td>
      <td>s__viridiflava</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
    </tr>
    <tr>
      <th>925397</th>
      <td>k__Bacteria</td>
      <td>p__Actinobacteria</td>
      <td>c__Actinobacteria</td>
      <td>o__Actinomycetales</td>
      <td>f__Propionibacteriaceae</td>
      <td>g__Propionibacterium</td>
      <td>s__acnes</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
    </tr>
    <tr>
      <th>4403262</th>
      <td>k__Bacteria</td>
      <td>p__Actinobacteria</td>
      <td>c__Actinobacteria</td>
      <td>o__Actinomycetales</td>
      <td>f__Propionibacteriaceae</td>
      <td>g__Propionibacterium</td>
      <td>s__acnes</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
    </tr>
    <tr>
      <th>165421</th>
      <td>k__Bacteria</td>
      <td>p__Actinobacteria</td>
      <td>c__Actinobacteria</td>
      <td>o__Actinomycetales</td>
      <td>f__Propionibacteriaceae</td>
      <td>g__Propionibacterium</td>
      <td>s__acnes</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
    </tr>
    <tr>
      <th>2028260</th>
      <td>k__Bacteria</td>
      <td>p__Actinobacteria</td>
      <td>c__Actinobacteria</td>
      <td>o__Actinomycetales</td>
      <td>f__Propionibacteriaceae</td>
      <td>g__Propionibacterium</td>
      <td>s__acnes</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
    </tr>
    <tr>
      <th>4226754</th>
      <td>k__Bacteria</td>
      <td>p__Actinobacteria</td>
      <td>c__Actinobacteria</td>
      <td>o__Actinomycetales</td>
      <td>f__Propionibacteriaceae</td>
      <td>g__Propionibacterium</td>
      <td>s__acnes</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
    </tr>
    <tr>
      <th>527604</th>
      <td>k__Bacteria</td>
      <td>p__Bacteroidetes</td>
      <td>c__Bacteroidia</td>
      <td>o__Bacteroidales</td>
      <td>f__Prevotellaceae</td>
      <td>g__Prevotella</td>
      <td>s__copri</td>
      <td>Prevotella copri (Bacteroidetes)</td>
    </tr>
    <tr>
      <th>323619</th>
      <td>k__Bacteria</td>
      <td>p__Bacteroidetes</td>
      <td>c__Bacteroidia</td>
      <td>o__Bacteroidales</td>
      <td>f__Prevotellaceae</td>
      <td>g__Prevotella</td>
      <td>s__copri</td>
      <td>Prevotella copri (Bacteroidetes)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Our data is ready, so let’s get started with unsupervised learning.</p>
</div>
<div class="section" id="unsupervised-learning">
<h2>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Permalink to this headline">¶</a></h2>
<p>We’ll begin our exploration of machine learning approaches with unsupervised learning, and specifically with ordination. We’ll work through ordination in two strokes. First, we’ll explore an approach called <strong>Polar Ordination</strong>, where the math is simple but which isn’t widely used in practice because more informative techniques exist. Working through this on a small data set will give you an idea of how ordination techniques can reduce the dimensionality of a data set and how to interpret the results of an ordination. Then, we’ll apply an approach called <strong>Principal Coordinates Analysis (PCoA)</strong>. The math for PCoA is a bit more complicated than I want get into this book, but we’ll look at how to apply PCoA using scikit-bio.</p>
<div class="section" id="computing-distances-between-samples">
<h3>Computing distances between samples<a class="headerlink" href="#computing-distances-between-samples" title="Permalink to this headline">¶</a></h3>
<p>Most ordination techniques begin with computing <strong>distances</strong> between all pairs of samples. A simple and useful way to define distances between our samples is by computing the fraction of k-mers that are unique to either sample. This is known as the Jaccard Distance between the samples. This metric derives from set theory, and is the inverse of the Jaccard Index (or Jaccard Similarity). It is defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-jaccard-sim">
<span class="eqno">(1)<a class="headerlink" href="#equation-jaccard-sim" title="Permalink to this equation">¶</a></span>\[Jaccard \, Index_{(A,B)} = \frac{| A \cap B |}{| A \cup B |}\]</div>
<p>Let’s break this formula down. First, <span class="math notranslate nohighlight">\((A,B)\)</span> defines the two samples we’re computing distances between. We refer to them here with the variables <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. <span class="math notranslate nohighlight">\(| A \cap B |\)</span> is the count of features that are observed, or that have a value of one or more, in both samples. If you’ve studied set theory, you may recognize this as the size of the intersection of the sets of k-mers in samples <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. That number is divided by <span class="math notranslate nohighlight">\(| A \cup B |\)</span>, which is the count of features that are observed in either or both of the samples. In set theory terminology, this is the size of the union of the sets of k-mers in samples <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. The resulting value is a measure of the similarity of the two samples. To make this a distance, we simply subtract that value from 1.</p>
<div class="math notranslate nohighlight" id="equation-jaccard-dist">
<span class="eqno">(2)<a class="headerlink" href="#equation-jaccard-dist" title="Permalink to this equation">¶</a></span>\[Jaccard \, Distance_{(A,B)} = 1 - Jaccard \, Index_{(A,B)}\]</div>
<p>If we apply this computation to all pairs of samples in our feature table, we can store the results in a <strong>distance matrix</strong>. This can be computed from our feature table as follows using scikit-bio.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">skbio.diversity</span>

<span class="n">sequence_distance_matrix</span> <span class="o">=</span> <span class="n">skbio</span><span class="o">.</span><span class="n">diversity</span><span class="o">.</span><span class="n">beta_diversity</span><span class="p">(</span><span class="s1">&#39;jaccard&#39;</span><span class="p">,</span> <span class="n">sequence_feature_table</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="n">sequence_feature_table</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/share/miniconda/envs/iab2/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1776: DataConversionWarning: Data was converted to boolean for metric jaccard
  warnings.warn(msg, DataConversionWarning)
</pre></div>
</div>
</div>
</div>
<p>A convenient way to get an initial glance at patterns in a small dataset like the one we’re working with here would be to plot the distance matrix as a heatmap. Here, colors represent the distances and larger distances imply that a pair of samples are dissimilar from each other in their kmer content.</p>
<p>Notice that the values along the diagonal are zero: this is because the diagonal represents distances between a sequence and itself, which is always zero. In other words, <span class="math notranslate nohighlight">\(Jaccard \, Distance_{(A,A)} = 0\)</span>. Also notice that the matrix is symmetric, meaning that if you were to flip the values across the diagonal they would be equal to each other. In other words, <span class="math notranslate nohighlight">\(Jaccard \, Distance_{(A,B)} = Jaccard \, Distance_{(B,A)}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_distance_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_26_0.svg" src="_images/machine-learning_26_0.svg" /></div>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>If you refer back to the table of our sample labels (defined in the <code class="docutils literal notranslate"><span class="pre">sequence_labels</span></code> variable) above, do you notice any patterns emerging from this heatmap?</p>
</div>
<p>If you want to look up the distance between a specific pair of samples, you can do that as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_id1</span> <span class="o">=</span> <span class="n">sequence_feature_table</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sample_id2</span> <span class="o">=</span> <span class="n">sequence_feature_table</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">sequence_distance_matrix</span><span class="p">[</span><span class="n">sample_id1</span><span class="p">,</span> <span class="n">sample_id2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The Jaccard Distance between samples </span><span class="si">%s</span><span class="s1"> and </span><span class="si">%s</span><span class="s1"> is </span><span class="si">%1.3f</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">sample_id1</span><span class="p">,</span> <span class="n">sample_id2</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The Jaccard Distance between samples 272189 and 237364 is 0.162.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="polar-ordination">
<h3>Polar ordination<a class="headerlink" href="#polar-ordination" title="Permalink to this headline">¶</a></h3>
<p>Now that we have distances between all pairs of samples, we can perform a polar ordination on the samples. Results of ordination are typically viewed in a scatterplot with two or three dimensions, so I find it useful to think through polar ordination as an approach to build a scatterplot from this distance matrix.</p>
<p>First, identify the largest distance in the distance matrix and note the sample ids associated with that distance. We’ll refer to this distance as <span class="math notranslate nohighlight">\(D\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sequence_distance_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">sequence_distance_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">sample_id1</span> <span class="o">=</span> <span class="n">sequence_distance_matrix</span><span class="o">.</span><span class="n">ids</span><span class="p">[</span><span class="n">sorted_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">sample_id2</span> <span class="o">=</span> <span class="n">sequence_distance_matrix</span><span class="o">.</span><span class="n">ids</span><span class="p">[</span><span class="n">sorted_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">sequence_distance_matrix</span><span class="p">[</span><span class="n">sample_id1</span><span class="p">,</span> <span class="n">sample_id2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The largest distance in the distance matrix is </span><span class="si">%1.3f</span><span class="s1">, between samples </span><span class="si">%s</span><span class="s1"> and </span><span class="si">%s</span><span class="s1">.&#39;</span> <span class="o">%</span> 
    <span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">sample_id1</span><span class="p">,</span> <span class="n">sample_id2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The largest distance in the distance matrix is 0.354, between samples 165421 and 4447248.
</pre></div>
</div>
</div>
</div>
<p>These two samples define the first axis of your scatter plot. The length of this axis is <span class="math notranslate nohighlight">\(D\)</span>, and each sample will be placed at an endpoint on this axis. Choose one sample to be plotted at zero on this axis, and the other sample to plot at <span class="math notranslate nohighlight">\(D\)</span> on this axis. It doesn’t matter which sample you choose to plot at which endpoint.</p>
<p>Next, we’ll identify the location of every other samples on this first axis. For each sample <span class="math notranslate nohighlight">\(s\)</span>, this computed with the following formula.</p>
<div class="math notranslate nohighlight" id="equation-polar-ordination-axis">
<span class="eqno">(3)<a class="headerlink" href="#equation-polar-ordination-axis" title="Permalink to this equation">¶</a></span>\[A_s = \frac{D^2 + D1^2 - D2^2}{2 \times D}\]</div>
<p>In this formula, <span class="math notranslate nohighlight">\(A_s\)</span> is the location of the current sample on the current axis. <span class="math notranslate nohighlight">\(D\)</span> is the distance between the endpoints. <span class="math notranslate nohighlight">\(D1\)</span> is distance between the current sample and the sample at <span class="math notranslate nohighlight">\(0\)</span> on this axis, which you can look up in the distance matrix computed above. <span class="math notranslate nohighlight">\(D1\)</span> is distance between the current sample and the sample at <span class="math notranslate nohighlight">\(D\)</span> on this axis, which is also looked up in the distance matrix.</p>
<p>The following Python function can be applied to compute the placement of all samples on a polar ordination axis, given the distances between all pairs of samples and the identifiers of the samples serving as the endpoints of this axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_axis</span><span class="p">(</span><span class="n">dm</span><span class="p">,</span> <span class="n">endpoint1</span><span class="p">,</span> <span class="n">endpoint2</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">dm</span><span class="p">[</span><span class="n">endpoint1</span><span class="p">,</span> <span class="n">endpoint2</span><span class="p">]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="n">endpoint1</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">endpoint2</span><span class="p">:</span> <span class="n">d</span><span class="p">}</span>
    <span class="n">non_endpoints</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">dm</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">endpoint1</span><span class="p">,</span> <span class="n">endpoint2</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">non_endpoints</span><span class="p">:</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">dm</span><span class="p">[</span><span class="n">endpoint1</span><span class="p">,</span> <span class="n">e</span><span class="p">]</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="n">dm</span><span class="p">[</span><span class="n">endpoint2</span><span class="p">,</span> <span class="n">e</span><span class="p">]</span>
        <span class="n">result</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">d1</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">d2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axis1_values</span> <span class="o">=</span> <span class="n">compute_axis</span><span class="p">(</span><span class="n">sequence_distance_matrix</span><span class="p">,</span> 
                            <span class="n">sample_id1</span><span class="p">,</span>
                            <span class="n">sample_id2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this stage, we have computed our first polar ordination axis. If we sort and view this axis we may even be able to see some clustering or grouping of samples along this axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axis1_values</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>165421     0.000000
925397     0.021687
4403262    0.023686
4226754    0.027552
2028260    0.065344
142888     0.164283
260790     0.164400
4432796    0.170171
237364     0.181396
1146480    0.185722
89393      0.188902
398017     0.191782
976319     0.192352
321743     0.195518
1058276    0.198070
272189     0.198296
260809     0.198597
271891     0.198723
867450     0.199619
514723     0.200534
527604     0.208406
269937     0.215083
4393400    0.218425
522627     0.219049
198573     0.219710
1134389    0.221564
323619     0.225265
651897     0.226000
299770     0.230838
525243     0.233351
4307391    0.237457
1042479    0.239290
525942     0.239549
2904927    0.257522
4447248    0.353909
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>If you again refer back to the table of our sample labels (defined in the <code class="docutils literal notranslate"><span class="pre">sequence_labels</span></code> variable) above, do you notice any patterns in the ordered samples along this axis?</p>
</div>
<p>We can plot this single axis using a strip plot. In this plot, only placement on the horizontal axis is meaningful. The variation in placement of points along the vertical axis is only to aid in visualization. In this plot, each point represents a single sample from our feature table. Samples that are closer in space along this axis are more similar to each other in their k-mer composition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">axis1_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_37_0.png" src="_images/machine-learning_37_0.png" />
</div>
</div>
<p>While we may already be able to see some clustering of samples on the first axis, additional axes that are uncorrelated with this first axis can provide more information about which samples are most similar to each other.</p>
<p>Selecting the next axes to plot in polar ordination is more complicated than choosing the first. Generally, you would begin by computing all axes, which would be defined for all pairs of samples. Then, you identify the axes that represent the largest distances (as we did for our first axis above), but that are also uncorrelated with previously selected axes.</p>
<p>Let’s start this by computing all polar ordination axes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">polar_ordination</span><span class="p">(</span><span class="n">dm</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">id1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dm</span><span class="o">.</span><span class="n">ids</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">id2</span> <span class="ow">in</span> <span class="n">dm</span><span class="o">.</span><span class="n">ids</span><span class="p">[:</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">axis_label</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> to </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">id1</span><span class="p">,</span> <span class="n">id2</span><span class="p">)</span>
            <span class="n">result</span><span class="p">[</span><span class="n">axis_label</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_axis</span><span class="p">(</span><span class="n">dm</span><span class="p">,</span> <span class="n">id1</span><span class="p">,</span> <span class="n">id2</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<p>We can apply this function to our distance matrix, and see all of the polar ordination axes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_polar_ordination</span> <span class="o">=</span> <span class="n">polar_ordination</span><span class="p">(</span><span class="n">sequence_distance_matrix</span><span class="p">)</span>
<span class="n">sequence_polar_ordination</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>237364 to 272189</th>
      <th>1146480 to 272189</th>
      <th>1146480 to 237364</th>
      <th>89393 to 272189</th>
      <th>89393 to 237364</th>
      <th>89393 to 1146480</th>
      <th>4432796 to 272189</th>
      <th>4432796 to 237364</th>
      <th>4432796 to 1146480</th>
      <th>4432796 to 89393</th>
      <th>...</th>
      <th>198573 to 867450</th>
      <th>198573 to 4307391</th>
      <th>198573 to 4447248</th>
      <th>198573 to 1042479</th>
      <th>198573 to 525942</th>
      <th>198573 to 651897</th>
      <th>198573 to 299770</th>
      <th>198573 to 2904927</th>
      <th>198573 to 525243</th>
      <th>198573 to 269937</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1042479</th>
      <td>0.104613</td>
      <td>0.038908</td>
      <td>0.001746</td>
      <td>0.109122</td>
      <td>0.091462</td>
      <td>0.107343</td>
      <td>0.106301</td>
      <td>0.077864</td>
      <td>0.093084</td>
      <td>0.072497</td>
      <td>...</td>
      <td>0.059110</td>
      <td>0.127053</td>
      <td>0.088071</td>
      <td>0.177686</td>
      <td>0.112087</td>
      <td>0.112410</td>
      <td>0.069552</td>
      <td>0.095693</td>
      <td>0.071985</td>
      <td>0.093721</td>
    </tr>
    <tr>
      <th>1058276</th>
      <td>0.104779</td>
      <td>0.038851</td>
      <td>0.001423</td>
      <td>0.079497</td>
      <td>0.063219</td>
      <td>0.071780</td>
      <td>0.087912</td>
      <td>0.057771</td>
      <td>0.065605</td>
      <td>0.083284</td>
      <td>...</td>
      <td>0.211220</td>
      <td>0.157608</td>
      <td>0.101232</td>
      <td>0.121866</td>
      <td>0.144421</td>
      <td>0.090041</td>
      <td>0.035868</td>
      <td>0.064973</td>
      <td>0.017127</td>
      <td>0.087069</td>
    </tr>
    <tr>
      <th>1134389</th>
      <td>0.096491</td>
      <td>0.018539</td>
      <td>-0.005370</td>
      <td>0.058111</td>
      <td>0.050405</td>
      <td>0.060360</td>
      <td>0.098309</td>
      <td>0.077081</td>
      <td>0.097925</td>
      <td>0.117883</td>
      <td>...</td>
      <td>0.209320</td>
      <td>0.151261</td>
      <td>0.115701</td>
      <td>0.122645</td>
      <td>0.145743</td>
      <td>0.113444</td>
      <td>0.075340</td>
      <td>0.064588</td>
      <td>0.058820</td>
      <td>0.044973</td>
    </tr>
    <tr>
      <th>1146480</th>
      <td>0.081739</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.115400</td>
      <td>0.118023</td>
      <td>0.142241</td>
      <td>0.103649</td>
      <td>0.097182</td>
      <td>0.121212</td>
      <td>0.062792</td>
      <td>...</td>
      <td>0.077065</td>
      <td>0.103257</td>
      <td>0.076787</td>
      <td>0.067641</td>
      <td>0.115009</td>
      <td>0.046463</td>
      <td>-0.016077</td>
      <td>0.022596</td>
      <td>-0.015967</td>
      <td>0.023822</td>
    </tr>
    <tr>
      <th>142888</th>
      <td>0.089584</td>
      <td>0.012731</td>
      <td>-0.000009</td>
      <td>0.108947</td>
      <td>0.104835</td>
      <td>0.125535</td>
      <td>0.088305</td>
      <td>0.072938</td>
      <td>0.087742</td>
      <td>0.052299</td>
      <td>...</td>
      <td>0.065948</td>
      <td>0.054029</td>
      <td>0.044849</td>
      <td>0.034664</td>
      <td>0.075927</td>
      <td>0.028440</td>
      <td>-0.027593</td>
      <td>-0.030201</td>
      <td>-0.009058</td>
      <td>0.003444</td>
    </tr>
    <tr>
      <th>165421</th>
      <td>0.059791</td>
      <td>-0.026217</td>
      <td>0.009344</td>
      <td>0.117605</td>
      <td>0.139887</td>
      <td>0.163322</td>
      <td>0.064965</td>
      <td>0.076544</td>
      <td>0.084938</td>
      <td>0.016621</td>
      <td>...</td>
      <td>0.110241</td>
      <td>0.124205</td>
      <td>0.056433</td>
      <td>0.083909</td>
      <td>0.115649</td>
      <td>0.120682</td>
      <td>0.030844</td>
      <td>-0.002312</td>
      <td>0.021415</td>
      <td>0.072255</td>
    </tr>
    <tr>
      <th>198573</th>
      <td>0.064035</td>
      <td>0.008313</td>
      <td>0.036741</td>
      <td>0.108887</td>
      <td>0.127796</td>
      <td>0.128569</td>
      <td>0.062283</td>
      <td>0.069519</td>
      <td>0.052436</td>
      <td>0.022887</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2028260</th>
      <td>0.061129</td>
      <td>-0.019731</td>
      <td>0.013620</td>
      <td>0.126825</td>
      <td>0.147425</td>
      <td>0.169842</td>
      <td>0.066772</td>
      <td>0.077204</td>
      <td>0.082291</td>
      <td>0.008828</td>
      <td>...</td>
      <td>0.093890</td>
      <td>0.119329</td>
      <td>0.089770</td>
      <td>0.105482</td>
      <td>0.092151</td>
      <td>0.140837</td>
      <td>0.033838</td>
      <td>0.021274</td>
      <td>0.026975</td>
      <td>0.073321</td>
    </tr>
    <tr>
      <th>237364</th>
      <td>0.000000</td>
      <td>-0.030977</td>
      <td>0.100877</td>
      <td>0.103375</td>
      <td>0.180258</td>
      <td>0.149567</td>
      <td>0.095251</td>
      <td>0.167382</td>
      <td>0.134198</td>
      <td>0.066114</td>
      <td>...</td>
      <td>0.067473</td>
      <td>0.112193</td>
      <td>0.059466</td>
      <td>0.047774</td>
      <td>0.116096</td>
      <td>0.033467</td>
      <td>-0.018740</td>
      <td>0.020323</td>
      <td>-0.019288</td>
      <td>0.035418</td>
    </tr>
    <tr>
      <th>260790</th>
      <td>0.082421</td>
      <td>0.010305</td>
      <td>0.009119</td>
      <td>0.110251</td>
      <td>0.112526</td>
      <td>0.128808</td>
      <td>0.069926</td>
      <td>0.059966</td>
      <td>0.062233</td>
      <td>0.030089</td>
      <td>...</td>
      <td>0.074806</td>
      <td>0.087055</td>
      <td>0.050475</td>
      <td>0.037177</td>
      <td>0.100154</td>
      <td>0.032709</td>
      <td>-0.019903</td>
      <td>-0.045449</td>
      <td>-0.020669</td>
      <td>-0.021965</td>
    </tr>
    <tr>
      <th>260809</th>
      <td>0.090921</td>
      <td>0.006477</td>
      <td>-0.008360</td>
      <td>0.072710</td>
      <td>0.069267</td>
      <td>0.086384</td>
      <td>0.056254</td>
      <td>0.036899</td>
      <td>0.044926</td>
      <td>0.054666</td>
      <td>...</td>
      <td>0.070042</td>
      <td>0.125359</td>
      <td>0.111474</td>
      <td>0.049904</td>
      <td>0.121872</td>
      <td>0.068301</td>
      <td>-0.015107</td>
      <td>-0.012311</td>
      <td>0.011612</td>
      <td>-0.002003</td>
    </tr>
    <tr>
      <th>269937</th>
      <td>0.051987</td>
      <td>0.003433</td>
      <td>0.051298</td>
      <td>0.100730</td>
      <td>0.130915</td>
      <td>0.122198</td>
      <td>0.068647</td>
      <td>0.088107</td>
      <td>0.065989</td>
      <td>0.038801</td>
      <td>...</td>
      <td>0.026293</td>
      <td>0.067152</td>
      <td>0.030296</td>
      <td>0.066795</td>
      <td>0.072511</td>
      <td>0.055498</td>
      <td>0.053343</td>
      <td>0.111689</td>
      <td>-0.005028</td>
      <td>0.126638</td>
    </tr>
    <tr>
      <th>271891</th>
      <td>0.103593</td>
      <td>0.015815</td>
      <td>-0.019504</td>
      <td>0.086794</td>
      <td>0.071207</td>
      <td>0.096745</td>
      <td>0.077554</td>
      <td>0.047693</td>
      <td>0.069105</td>
      <td>0.063763</td>
      <td>...</td>
      <td>0.105826</td>
      <td>0.095628</td>
      <td>0.067209</td>
      <td>0.057395</td>
      <td>0.138400</td>
      <td>0.009220</td>
      <td>-0.058591</td>
      <td>-0.004521</td>
      <td>-0.048121</td>
      <td>0.021909</td>
    </tr>
    <tr>
      <th>272189</th>
      <td>0.162393</td>
      <td>0.100000</td>
      <td>-0.030707</td>
      <td>0.170940</td>
      <td>0.098031</td>
      <td>0.138684</td>
      <td>0.181435</td>
      <td>0.103248</td>
      <td>0.155145</td>
      <td>0.091630</td>
      <td>...</td>
      <td>0.074073</td>
      <td>0.113421</td>
      <td>0.080808</td>
      <td>0.084860</td>
      <td>0.117205</td>
      <td>0.055708</td>
      <td>-0.021216</td>
      <td>0.030584</td>
      <td>0.006388</td>
      <td>0.019969</td>
    </tr>
    <tr>
      <th>2904927</th>
      <td>0.073295</td>
      <td>0.020018</td>
      <td>0.033438</td>
      <td>0.123202</td>
      <td>0.133029</td>
      <td>0.137543</td>
      <td>0.097878</td>
      <td>0.099120</td>
      <td>0.096060</td>
      <td>0.047930</td>
      <td>...</td>
      <td>0.027110</td>
      <td>0.079506</td>
      <td>0.052600</td>
      <td>0.078925</td>
      <td>0.077639</td>
      <td>0.061873</td>
      <td>0.090158</td>
      <td>0.146552</td>
      <td>0.022062</td>
      <td>0.129252</td>
    </tr>
    <tr>
      <th>299770</th>
      <td>0.062692</td>
      <td>0.003785</td>
      <td>0.034415</td>
      <td>0.111501</td>
      <td>0.131485</td>
      <td>0.134894</td>
      <td>0.059626</td>
      <td>0.067943</td>
      <td>0.052195</td>
      <td>0.017087</td>
      <td>...</td>
      <td>0.006742</td>
      <td>0.019925</td>
      <td>0.026841</td>
      <td>0.034488</td>
      <td>0.011369</td>
      <td>0.060574</td>
      <td>0.088106</td>
      <td>0.054203</td>
      <td>0.036355</td>
      <td>0.037113</td>
    </tr>
    <tr>
      <th>321743</th>
      <td>0.083549</td>
      <td>0.024470</td>
      <td>0.021343</td>
      <td>0.138456</td>
      <td>0.138257</td>
      <td>0.152746</td>
      <td>0.114753</td>
      <td>0.107463</td>
      <td>0.117647</td>
      <td>0.050765</td>
      <td>...</td>
      <td>0.051047</td>
      <td>0.086344</td>
      <td>0.053746</td>
      <td>0.056977</td>
      <td>0.055843</td>
      <td>0.053099</td>
      <td>-0.002443</td>
      <td>0.003831</td>
      <td>-0.011321</td>
      <td>0.011751</td>
    </tr>
    <tr>
      <th>323619</th>
      <td>0.064164</td>
      <td>0.023629</td>
      <td>0.051717</td>
      <td>0.118871</td>
      <td>0.137148</td>
      <td>0.129800</td>
      <td>0.070743</td>
      <td>0.078565</td>
      <td>0.052464</td>
      <td>0.021815</td>
      <td>...</td>
      <td>0.059818</td>
      <td>0.104198</td>
      <td>0.075287</td>
      <td>0.092214</td>
      <td>0.084712</td>
      <td>0.078266</td>
      <td>0.040978</td>
      <td>0.016075</td>
      <td>0.023794</td>
      <td>0.034820</td>
    </tr>
    <tr>
      <th>398017</th>
      <td>0.126771</td>
      <td>0.034405</td>
      <td>-0.038387</td>
      <td>0.106703</td>
      <td>0.069206</td>
      <td>0.107601</td>
      <td>0.094201</td>
      <td>0.043251</td>
      <td>0.078687</td>
      <td>0.061373</td>
      <td>...</td>
      <td>0.070748</td>
      <td>0.078457</td>
      <td>0.063634</td>
      <td>0.031661</td>
      <td>0.083340</td>
      <td>0.025372</td>
      <td>-0.013879</td>
      <td>-0.011114</td>
      <td>-0.000352</td>
      <td>-0.014344</td>
    </tr>
    <tr>
      <th>4226754</th>
      <td>0.072712</td>
      <td>0.012349</td>
      <td>0.026773</td>
      <td>0.092567</td>
      <td>0.104503</td>
      <td>0.106120</td>
      <td>0.064417</td>
      <td>0.063414</td>
      <td>0.052301</td>
      <td>0.042721</td>
      <td>...</td>
      <td>0.156528</td>
      <td>0.146422</td>
      <td>0.075733</td>
      <td>0.083935</td>
      <td>0.137786</td>
      <td>0.106248</td>
      <td>0.030814</td>
      <td>0.029303</td>
      <td>0.072318</td>
      <td>0.089084</td>
    </tr>
    <tr>
      <th>4307391</th>
      <td>0.065362</td>
      <td>0.026151</td>
      <td>0.052288</td>
      <td>0.097376</td>
      <td>0.115685</td>
      <td>0.102195</td>
      <td>0.096237</td>
      <td>0.105037</td>
      <td>0.088543</td>
      <td>0.073633</td>
      <td>...</td>
      <td>0.095022</td>
      <td>0.175510</td>
      <td>0.113241</td>
      <td>0.125498</td>
      <td>0.149368</td>
      <td>0.118884</td>
      <td>0.039692</td>
      <td>0.095217</td>
      <td>0.040296</td>
      <td>0.093068</td>
    </tr>
    <tr>
      <th>4393400</th>
      <td>0.064028</td>
      <td>0.008412</td>
      <td>0.036850</td>
      <td>0.052398</td>
      <td>0.074233</td>
      <td>0.060613</td>
      <td>0.054260</td>
      <td>0.060830</td>
      <td>0.040346</td>
      <td>0.074085</td>
      <td>...</td>
      <td>0.161822</td>
      <td>0.127110</td>
      <td>0.052718</td>
      <td>0.101982</td>
      <td>0.123265</td>
      <td>0.010247</td>
      <td>0.001259</td>
      <td>0.066974</td>
      <td>0.009344</td>
      <td>0.061552</td>
    </tr>
    <tr>
      <th>4403262</th>
      <td>0.060035</td>
      <td>-0.005838</td>
      <td>0.029153</td>
      <td>0.130190</td>
      <td>0.151602</td>
      <td>0.164120</td>
      <td>0.054201</td>
      <td>0.064640</td>
      <td>0.052014</td>
      <td>-0.009003</td>
      <td>...</td>
      <td>0.092984</td>
      <td>0.099362</td>
      <td>0.043154</td>
      <td>0.037950</td>
      <td>0.070220</td>
      <td>0.077949</td>
      <td>-0.037525</td>
      <td>-0.010834</td>
      <td>-0.007486</td>
      <td>-0.009542</td>
    </tr>
    <tr>
      <th>4432796</th>
      <td>0.066104</td>
      <td>-0.041131</td>
      <td>-0.015604</td>
      <td>0.064226</td>
      <td>0.083579</td>
      <td>0.109658</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.069494</td>
      <td>0.078321</td>
      <td>0.038704</td>
      <td>0.039913</td>
      <td>0.114799</td>
      <td>-0.000014</td>
      <td>-0.015745</td>
      <td>-0.013484</td>
      <td>-0.029580</td>
      <td>0.010850</td>
    </tr>
    <tr>
      <th>4447248</th>
      <td>0.096623</td>
      <td>0.018283</td>
      <td>-0.005836</td>
      <td>0.137054</td>
      <td>0.125149</td>
      <td>0.155410</td>
      <td>0.119827</td>
      <td>0.100278</td>
      <td>0.130344</td>
      <td>0.058008</td>
      <td>...</td>
      <td>0.078825</td>
      <td>0.159992</td>
      <td>0.247967</td>
      <td>0.122906</td>
      <td>0.154425</td>
      <td>0.135936</td>
      <td>0.075542</td>
      <td>0.088999</td>
      <td>0.058748</td>
      <td>0.059323</td>
    </tr>
    <tr>
      <th>514723</th>
      <td>0.062242</td>
      <td>-0.015442</td>
      <td>0.016078</td>
      <td>0.101521</td>
      <td>0.122426</td>
      <td>0.136418</td>
      <td>0.068151</td>
      <td>0.077620</td>
      <td>0.080819</td>
      <td>0.037395</td>
      <td>...</td>
      <td>0.042482</td>
      <td>0.108550</td>
      <td>0.080603</td>
      <td>0.077458</td>
      <td>0.083677</td>
      <td>0.059302</td>
      <td>-0.010600</td>
      <td>0.058611</td>
      <td>0.072064</td>
      <td>0.051497</td>
    </tr>
    <tr>
      <th>522627</th>
      <td>0.084714</td>
      <td>0.036715</td>
      <td>0.031607</td>
      <td>0.113002</td>
      <td>0.113069</td>
      <td>0.113547</td>
      <td>0.039448</td>
      <td>0.024705</td>
      <td>-0.005176</td>
      <td>-0.007371</td>
      <td>...</td>
      <td>0.047699</td>
      <td>0.044781</td>
      <td>0.043853</td>
      <td>0.019518</td>
      <td>0.035258</td>
      <td>0.037617</td>
      <td>0.041415</td>
      <td>-0.001647</td>
      <td>0.045098</td>
      <td>0.004919</td>
    </tr>
    <tr>
      <th>525243</th>
      <td>0.084480</td>
      <td>0.037220</td>
      <td>0.032484</td>
      <td>0.101064</td>
      <td>0.101959</td>
      <td>0.098846</td>
      <td>0.087917</td>
      <td>0.077470</td>
      <td>0.066958</td>
      <td>0.060273</td>
      <td>...</td>
      <td>0.010900</td>
      <td>0.029689</td>
      <td>0.030636</td>
      <td>0.052387</td>
      <td>0.028072</td>
      <td>0.038330</td>
      <td>0.053358</td>
      <td>0.019467</td>
      <td>0.129310</td>
      <td>-0.005134</td>
    </tr>
    <tr>
      <th>525942</th>
      <td>0.065271</td>
      <td>0.012289</td>
      <td>0.038692</td>
      <td>0.079685</td>
      <td>0.098990</td>
      <td>0.090680</td>
      <td>0.064684</td>
      <td>0.070922</td>
      <td>0.052750</td>
      <td>0.056772</td>
      <td>...</td>
      <td>0.100877</td>
      <td>0.154100</td>
      <td>0.112764</td>
      <td>0.114222</td>
      <td>0.181070</td>
      <td>0.087698</td>
      <td>0.023364</td>
      <td>0.095926</td>
      <td>0.039308</td>
      <td>0.103678</td>
    </tr>
    <tr>
      <th>527604</th>
      <td>0.084778</td>
      <td>0.018560</td>
      <td>0.013507</td>
      <td>0.090604</td>
      <td>0.091771</td>
      <td>0.099394</td>
      <td>0.048497</td>
      <td>0.034452</td>
      <td>0.023348</td>
      <td>0.026783</td>
      <td>...</td>
      <td>0.074977</td>
      <td>0.063384</td>
      <td>0.054915</td>
      <td>0.036643</td>
      <td>0.035476</td>
      <td>0.050643</td>
      <td>0.024614</td>
      <td>-0.014567</td>
      <td>0.061849</td>
      <td>-0.006454</td>
    </tr>
    <tr>
      <th>651897</th>
      <td>0.084020</td>
      <td>0.021803</td>
      <td>0.017942</td>
      <td>0.110377</td>
      <td>0.111205</td>
      <td>0.120876</td>
      <td>0.107098</td>
      <td>0.098708</td>
      <td>0.108388</td>
      <td>0.072061</td>
      <td>...</td>
      <td>0.035601</td>
      <td>0.098843</td>
      <td>0.079995</td>
      <td>0.092316</td>
      <td>0.070675</td>
      <td>0.145923</td>
      <td>0.100324</td>
      <td>0.061607</td>
      <td>0.043254</td>
      <td>0.063949</td>
    </tr>
    <tr>
      <th>867450</th>
      <td>0.073234</td>
      <td>0.001540</td>
      <td>0.015218</td>
      <td>0.079483</td>
      <td>0.091624</td>
      <td>0.097994</td>
      <td>0.067995</td>
      <td>0.066786</td>
      <td>0.066574</td>
      <td>0.060738</td>
      <td>...</td>
      <td>0.226337</td>
      <td>0.122540</td>
      <td>0.071949</td>
      <td>0.075295</td>
      <td>0.126096</td>
      <td>0.055220</td>
      <td>0.017321</td>
      <td>0.041869</td>
      <td>0.019079</td>
      <td>0.046994</td>
    </tr>
    <tr>
      <th>89393</th>
      <td>0.091272</td>
      <td>0.005060</td>
      <td>-0.010329</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.080893</td>
      <td>0.063267</td>
      <td>0.082975</td>
      <td>0.160173</td>
      <td>...</td>
      <td>0.096280</td>
      <td>0.124632</td>
      <td>0.061390</td>
      <td>0.084634</td>
      <td>0.144773</td>
      <td>0.053962</td>
      <td>-0.026288</td>
      <td>0.013886</td>
      <td>0.016728</td>
      <td>0.030979</td>
    </tr>
    <tr>
      <th>925397</th>
      <td>0.046857</td>
      <td>-0.047199</td>
      <td>0.009364</td>
      <td>0.118180</td>
      <td>0.152084</td>
      <td>0.178764</td>
      <td>0.076260</td>
      <td>0.101336</td>
      <td>0.119156</td>
      <td>0.028802</td>
      <td>...</td>
      <td>0.110204</td>
      <td>0.124129</td>
      <td>0.085078</td>
      <td>0.095848</td>
      <td>0.104190</td>
      <td>0.134405</td>
      <td>0.055300</td>
      <td>0.013726</td>
      <td>0.021543</td>
      <td>0.072113</td>
    </tr>
    <tr>
      <th>976319</th>
      <td>0.094679</td>
      <td>0.006102</td>
      <td>-0.014782</td>
      <td>0.089469</td>
      <td>0.081774</td>
      <td>0.106788</td>
      <td>0.078685</td>
      <td>0.057567</td>
      <td>0.078811</td>
      <td>0.062189</td>
      <td>...</td>
      <td>0.173543</td>
      <td>0.148421</td>
      <td>0.094144</td>
      <td>0.121317</td>
      <td>0.158759</td>
      <td>0.133256</td>
      <td>0.093108</td>
      <td>0.077179</td>
      <td>0.045790</td>
      <td>0.073706</td>
    </tr>
  </tbody>
</table>
<p>35 rows × 595 columns</p>
</div></div></div>
</div>
<p>This isn’t much (or any) more interpretable than the distance matrix itself was, so the next step is to select the most informative axes to view. The first (most informative) axis will still be the axis that we identified above as the one representing the largest distance in the distance matrix. The second will be an axis that also contains a large distance (relative to the other distances in the distance matrix), and which is uncorrelated with the first axis. Selecting this axis based on these two criteria is subjective, because there is not a specific definition of “uncorrelated”. We need to come up with an objective approach so a computer can solve the problem. Here, I define a <em>score</em> metric that can be computed for each axis. This score is computed as the largest distance along the current axis divided by the absolute value of the Spearman correlation coefficient between the current axis and the first axis. In other words:</p>
<div class="math notranslate nohighlight" id="equation-polar-ordination-axis-score">
<span class="eqno">(4)<a class="headerlink" href="#equation-polar-ordination-axis-score" title="Permalink to this equation">¶</a></span>\[Score_{A} = \frac{max \, distance \, on \, A}{|Spearman(Axis \, 1, A)|}\]</div>
<p>In this formula, <span class="math notranslate nohighlight">\(A\)</span> represents the current axis, and <span class="math notranslate nohighlight">\(Axis 1\)</span> represents the first axis in the polar ordination.</p>
<p>This can be computed as follows. This function takes a polar ordination result as input and returns the identifiers of the first two axes as well as a summary of the maximum distance along each axis, the correlation of each axis with the first axis, and the score (as defined by <a class="reference internal" href="#equation-polar-ordination-axis-score">(4)</a>) for each axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">select_polar_ordination_axes</span><span class="p">(</span><span class="n">polar_ordination</span><span class="p">):</span>
    <span class="c1"># this function would be better if it defined more axes, </span>
    <span class="c1"># eg by always looking for correlation with preceding axis. </span>
    <span class="n">distance_sorted_ord_axes</span> <span class="o">=</span> <span class="n">polar_ordination</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">first_axis_idx</span> <span class="o">=</span> <span class="n">distance_sorted_ord_axes</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">corrs</span> <span class="o">=</span> <span class="n">polar_ordination</span><span class="o">.</span><span class="n">corrwith</span><span class="p">(</span><span class="n">polar_ordination</span><span class="p">[</span><span class="n">first_axis_idx</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;spearman&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">distance_sorted_ord_axes</span> <span class="o">/</span> <span class="n">corrs</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">distance_sorted_ord_axes</span><span class="p">,</span> <span class="n">corrs</span><span class="p">,</span> <span class="n">scores</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;maximum distance&#39;</span><span class="p">,</span> <span class="s1">&#39;corr with first axis&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">]</span>
    <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">second_axis_idx</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">first_axis_idx</span><span class="p">,</span> <span class="n">second_axis_idx</span><span class="p">,</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">first_axis_idx</span><span class="p">,</span> <span class="n">second_axis_idx</span><span class="p">,</span> <span class="n">axis_summaries</span> <span class="o">=</span> <span class="n">select_polar_ordination_axes</span><span class="p">(</span><span class="n">sequence_polar_ordination</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Axis 1 is defined by the distance from sample </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">first_axis_idx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Axis 2 is defined by the distance from sample </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">second_axis_idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Axis 1 is defined by the distance from sample 4447248 to 165421
Axis 2 is defined by the distance from sample 398017 to 237364
</pre></div>
</div>
</div>
</div>
<p>For each axis, we can view the maximum distance along that axis, it’s correlation coefficient with the first axis, and the score of the axis, by viewing the summary returned from <code class="docutils literal notranslate"><span class="pre">select_polar_ordination_axes</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axis_summaries</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>maximum distance</th>
      <th>corr with first axis</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>398017 to 237364</th>
      <td>0.175214</td>
      <td>0.001120</td>
      <td>156.378205</td>
    </tr>
    <tr>
      <th>651897 to 522627</th>
      <td>0.167382</td>
      <td>0.003922</td>
      <td>42.682403</td>
    </tr>
    <tr>
      <th>4432796 to 1146480</th>
      <td>0.155145</td>
      <td>0.007283</td>
      <td>21.302592</td>
    </tr>
    <tr>
      <th>4307391 to 522627</th>
      <td>0.181070</td>
      <td>0.008684</td>
      <td>20.850790</td>
    </tr>
    <tr>
      <th>269937 to 4447248</th>
      <td>0.250000</td>
      <td>0.020448</td>
      <td>12.226027</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>198573 to 299770</th>
      <td>0.100324</td>
      <td>0.543417</td>
      <td>0.184617</td>
    </tr>
    <tr>
      <th>522627 to 527604</th>
      <td>0.092920</td>
      <td>0.569508</td>
      <td>0.163159</td>
    </tr>
    <tr>
      <th>2028260 to 165421</th>
      <td>0.097561</td>
      <td>0.769015</td>
      <td>0.126865</td>
    </tr>
    <tr>
      <th>2028260 to 925397</th>
      <td>0.086009</td>
      <td>0.680440</td>
      <td>0.126401</td>
    </tr>
    <tr>
      <th>269937 to 2904927</th>
      <td>0.075219</td>
      <td>0.782073</td>
      <td>0.096179</td>
    </tr>
  </tbody>
</table>
<p>595 rows × 3 columns</p>
</div></div></div>
</div>
<p>Now we can expand our strip plot to a two-dimensional scatter plot where we plot the first two axes of our polar ordination. You should notice that there is some grouping or clustering of samples in this plot. This group should be more distinct than it was in the strip plot, since our samples can now separate along two axes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">first_axis_idx</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">second_axis_idx</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_48_0.png" src="_images/machine-learning_48_0.png" />
</div>
</div>
<p>This plot illustrates that there is some structure in our dataset. If you look up distances between samples that are closer to each other in space in the scatterplot, those distances on average will be smaller than the distances between samples that are farther apart in the scatterplot. This structure in the dataset is what has been “learned” by the polar ordination algorithm. Notice that the sequence labels were not used at all in this analysis, but if you look up where each sample is found on the plot, and cross-reference that against the sample labels, you’ll discover that samples that cluster together are from the same species.</p>
<p>When labels are available for a collection of samples, labels can be used with an ordination plot to explore whether samples cluster by sample label. This is often achieved by coloring the points in a scatterplot by their label. We can do this for our sequence ordination as follows. Notice that the colors (the microbial species represented in our dataset) group together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">first_axis_idx</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">second_axis_idx</span><span class="p">],</span> 
                    <span class="n">hue</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_50_0.png" src="_images/machine-learning_50_0.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember that while we used the sample labels to help us interpret the ordination plot, they were not used at all in computing the ordination. Not using the labels when computing the ordination is what defines this ordination approach as being unsupervised.</p>
</div>
</div>
<div class="section" id="interpreting-ordination-plots">
<h3>Interpreting ordination plots<a class="headerlink" href="#interpreting-ordination-plots" title="Permalink to this headline">¶</a></h3>
<p>Let’s now work through some of the ideas we explored with polar ordination to understand general features of unsupervised ordination plots.</p>
<div class="section" id="axis-order">
<h4>Axis order<a class="headerlink" href="#axis-order" title="Permalink to this headline">¶</a></h4>
<p>First, the order of the axes in an ordination is generally important. The first axis typically represents in the largest differences in the data set, and the second axis generally presents the next largest differences that are uncorrelated with the first axis. If we didn’t select axes that represented the largest distances, we would be less likely to see patterns in our data.</p>
<p>For example, let’s focus on just a single axis again. We can sort our axis summary by maximum distance along an axis, and plot the axes that represent the largest and then the smallest distances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distance_sorted_axis_summary</span> <span class="o">=</span> <span class="n">axis_summaries</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;maximum distance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>First, we’ll plot our samples along the axis representing the largest distance and we’ll separate the samples by species so we can see how their placement along the axes we define differ. You should notice here that the samples within each species roughly group together, and there may even be some separation of species on the axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">largest_distance_axis_idx</span> <span class="o">=</span> <span class="n">distance_sorted_axis_summary</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">largest_distance_axis_idx</span><span class="p">],</span> 
                  <span class="n">y</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_54_0.png" src="_images/machine-learning_54_0.png" />
</div>
</div>
<p>Now contrast this with what we’d see if we generated this same plot, but based on the smallest distance in the distance matrix rather than the largest. Clustering of samples by species should be much less apparent here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">smallest_distance_axis_idx</span> <span class="o">=</span> <span class="n">distance_sorted_axis_summary</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">smallest_distance_axis_idx</span><span class="p">],</span> 
                  <span class="n">y</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_56_0.png" src="_images/machine-learning_56_0.png" />
</div>
</div>
<p>As you plot successive axes in an ordination, the axes represent smaller differences between samples. Typically you’ll want to focus on the first few ordination axes to the exclusion of later axes.</p>
</div>
<div class="section" id="uncorrelated-axes">
<h4>Uncorrelated axes<a class="headerlink" href="#uncorrelated-axes" title="Permalink to this headline">¶</a></h4>
<p>Next, ordination algorithms generally present uncorrelated axes as the most important axes. This is important because correlated axes, by definition, will present similar information.</p>
<p>Let’s look at another example here where we’ll plot a second polar ordination axis that is highly correlated with the first axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the axis most correlated with first_axis_idx will be first_axis_idx, so </span>
<span class="c1"># select the second axis</span>
<span class="n">correlated_axis_idx</span> <span class="o">=</span> <span class="n">axis_summaries</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;corr with first axis&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">first_axis_idx</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_polar_ordination</span><span class="p">[</span><span class="n">correlated_axis_idx</span><span class="p">],</span> 
                    <span class="n">hue</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_59_0.png" src="_images/machine-learning_59_0.png" />
</div>
</div>
<p>You can see that we don’t get much additional information from the second axis than we do from the first. In fact, if we drew a line along the diagonal, the placement of points along that line would be almost identical to those that we observed in the strip plot of the first axis that we started with above. The correlated axis isn’t adding much new information to our understanding of the clustering of samples, so we might as well not have it.</p>
</div>
<div class="section" id="directionality-of-the-axes">
<h4>Directionality of the axes<a class="headerlink" href="#directionality-of-the-axes" title="Permalink to this headline">¶</a></h4>
<p>One thing that you may have noticed as you computed the polar ordination above is that our definitions of end points was arbitrary. We first encountered this when computing <a class="reference internal" href="#equation-polar-ordination-axis">(3)</a>, when we defined one of a pair of samples to be placed at <span class="math notranslate nohighlight">\(0\)</span> along an axis, and the other of the pair of samples to be placed at <span class="math notranslate nohighlight">\(D\)</span> along the axis. Let’s again look at placement of all samples along the axis represented the largest distance in our data set computed as we did initially.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axis1_values_a</span> <span class="o">=</span> <span class="n">compute_axis</span><span class="p">(</span><span class="n">sequence_distance_matrix</span><span class="p">,</span> 
                              <span class="n">sample_id1</span><span class="p">,</span>
                              <span class="n">sample_id2</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">axis1_values_a</span><span class="p">,</span> 
                  <span class="n">y</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_61_0.png" src="_images/machine-learning_61_0.png" />
</div>
</div>
<p>Now, let’s reverse the order of the sample ids that we’re providing as input to this function. In practice, this means that the sample that was previously placed at <span class="math notranslate nohighlight">\(0\)</span> will now be placed at <span class="math notranslate nohighlight">\(D\)</span> along this axis, and the sample that was previously placed at <span class="math notranslate nohighlight">\(D\)</span> will now be placed at <span class="math notranslate nohighlight">\(0\)</span> along this axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axis1_values_b</span> <span class="o">=</span> <span class="n">compute_axis</span><span class="p">(</span><span class="n">sequence_distance_matrix</span><span class="p">,</span> 
                              <span class="n">sample_id2</span><span class="p">,</span>
                              <span class="n">sample_id1</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">axis1_values_b</span><span class="p">,</span> 
                  <span class="n">y</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_63_0.png" src="_images/machine-learning_63_0.png" />
</div>
</div>
<p>Notice that these two plots are mirror images of each other. Because they’re perfectly anti-correlated, they present identical information about the grouping of the samples. This will be true for any axis in our ordination, and for this reason the directionality of the axes in an ordination is not meaningful. You can always flip an axis and have the same result. You may also notice that if you run the same ordination multiple times, the directionality of the axes might change across runs. This is typically a result of how the algorithm is implemented, and it doesn’t impact your results at all.</p>
</div>
</div>
<div class="section" id="principle-coordinates-analysis-pcoa">
<h3>Principle Coordinates Analysis (PCoA)<a class="headerlink" href="#principle-coordinates-analysis-pcoa" title="Permalink to this headline">¶</a></h3>
<p>Finally, lets conclude our introduction to unsupervised learning by plotting these same data using principle coordinates analysis, or PCoA. As mentioned earlier, the math for PCoA is more complex than for polar ordination, but it works better than polar ordination and should be preferred in practice to polar ordination. scikit-bio includes an implementation of PCoA that can be used in practice as illustrated here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">skbio.stats.ordination</span>
<span class="n">sequence_pcoa_ordination</span> <span class="o">=</span> <span class="n">skbio</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">ordination</span><span class="o">.</span><span class="n">pcoa</span><span class="p">(</span><span class="n">sequence_distance_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/share/miniconda/envs/iab2/lib/python3.8/site-packages/skbio/stats/ordination/_principal_coordinate_analysis.py:143: RuntimeWarning: The result contains negative eigenvalues. Please compare their magnitude with the magnitude of some of the largest positive eigenvalues. If the negative ones are smaller, it&#39;s probably safe to ignore them, but if they are large in magnitude, the results won&#39;t be useful. See the Notes section for more details. The smallest eigenvalue is -0.007900897745463603 and the largest is 0.2588414600820823.
  warn(
</pre></div>
</div>
</div>
</div>
<p>Just as with polar ordination, we can view a 2D scatterplot of these data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC2&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_67_0.png" src="_images/machine-learning_67_0.png" />
</div>
</div>
<p>That plot becomes more informative when we integrate sample labels, but like polar ordination, those sample labels were not used in the PCoA computation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC2&#39;</span><span class="p">],</span> 
                    <span class="n">hue</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_69_0.png" src="_images/machine-learning_69_0.png" />
</div>
</div>
<p>PCoA (and other) ordination plots are often used in exploratory analysis, for example to see if sample similarity is associated with categories of metadata. This can be achieved by altering the metadata category that is used to define the coloring of samples in the plot. For example, in the following plot samples are colored by phlyum. Comparing this with the plot above, where samples are colored by genus and species, illustrates that samples cluster more distinctly by phylum than by species.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC2&#39;</span><span class="p">],</span> 
                    <span class="n">hue</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;phylum&#39;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_71_0.png" src="_images/machine-learning_71_0.png" />
</div>
</div>
<p>While in the examples presented here we have looked at the first two ordination axes, ordination methods typically generate many axes. The exact number of axes differs by method, but it’s generally a function of the number of samples in the analysis. Ordination axes are typically ordered by the amount of variation in the data that is explained, so the first axis explains more varation than the second; the second axis explains more variation than the third; and so on. For that reason, visualization of ordination plots typically focus on the first two or three axes. It’s possible to view additional axes however, and these may illustrate different patterns in your data. Typically however you will want to focus on the first few axes as the latter axis may be misleading if they explain relatively small amounts of variation. The following plot illustrates PCoA axis 1 versus PCoA axis 3, where the previous plots illustrate PCoA axis 1 versus PCoA axis 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">],</span> 
                    <span class="n">y</span><span class="o">=</span><span class="n">sequence_pcoa_ordination</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;PC3&#39;</span><span class="p">],</span> 
                    <span class="n">hue</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/machine-learning_73_0.png" src="_images/machine-learning_73_0.png" />
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Exploratory analysis of ordination data does not replace statistical comparisons of sample composition, and identifying a pattern in an ordination plot and subsequently testing whether it’s significant is not the same as having an a priori hypothesis about how your samples will group and then testing for that statistically. If you have many metadata categories, and especially if you have relatively few samples, it’s likely that spurious patterns may present themselves. <strong>You should consider observations that result from exploratory analysis of ordination plots as hypotheses that can subsequntly be tested with different data.</strong> Remember: exploratory analysis is a tool for hypothesis generation, and hypotheses generation and hypothesis testing cannot be performed on the same data.</p>
</div>
<p>Emperor <span id="id2">[<a class="reference internal" href="#id30">VazquezBPGK13</a>]</span> is a widely used tool for visualizing ordination plots, and it makes interactive exploratory analysis of PCoA plots very straight-forward. For example, coloring samples by different metadata categories, comparing different ordination axes, and inverting ordination axes are all possible to do with a few mouse clicks. Emperor is a great place to get started when doing your own exploratory analysis of ordination plots.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>How does the clustering of samples compare between polar ordination and PCoA?</p>
</div>
</div>
</div>
<div class="section" id="supervised-classification">
<h2>Supervised classification<a class="headerlink" href="#supervised-classification" title="Permalink to this headline">¶</a></h2>
<p>We’ll continue our exploration of machine learning approaches with <strong>supervised classification</strong>, and specifically with an algorithm called <strong>Naive Bayes</strong>.  We’ll implement Naive Bayes to gain an understanding of how it works. Like Polar Ordination, the math involved in Naive Bayes is relatively straight-forward, which is why I chose this algorithm to present here. There are many machine algorithms with more complex math, but Naive Bayes is widely used and powerful, so I think it’s a great place to get started.</p>
<p>We’ll explore supervised classification in the context of a now familiar topic: taxonomic classification of 16S rRNA sequences. We previously explored this problem in <a class="reference internal" href="database-searching.html"><span class="doc">Sequence homology searching</span></a>, so it’s worth spending a few minutes skimming that chapter if it’s not fresh in your mind.</p>
<p>Briefly, the problem that we are going to address here is as follows. We have a query sequence (<span class="math notranslate nohighlight">\(q_i\)</span>) which is not taxonomically annotated (meaning we don’t know the taxonomy of the organism whose genome it is found in), and a reference database (<span class="math notranslate nohighlight">\(R\)</span>) of taxonomically annotated sequences (<span class="math notranslate nohighlight">\(r_1, r_2, r_3, r_n\)</span>). We want to infer a taxonomic annotation for <span class="math notranslate nohighlight">\(q_i\)</span>. In <a class="reference internal" href="database-searching.html"><span class="doc">Sequence homology searching</span></a>, we solved this problem using pairwise sequence alignment. Here, we’ll build a Naive Bayes classifier from our sequence feature table and labels, and then apply that classifier to unlabeled data.</p>
<div class="section" id="defining-a-classification-task">
<h3>Defining a classification task<a class="headerlink" href="#defining-a-classification-task" title="Permalink to this headline">¶</a></h3>
<p>In a classification task, there are two or more pre-defined classes, and the goal is to assign observations to those classes. As humans, we perform these kinds of tasks everyday. For example, if you’re browsing a bookstore you might classify titles as ones you want to read versus everything else (the ones you’re not interested in reading). You might group the apps that you have on your phone into folders by classifying them by category (e.g., “school”, “entertainment”, or “social media”).</p>
<p>When we’re working with large data sets, supervised classification algorithms can help us with classification tasks that will make us more efficient or help us understand our data. A classic example of this outside of bioinformatics is an email spam filter. For every email that is received, the spam filter must define it as spam or not spam so the message can directed either to the user’s spam folder or the user’s inbox. The stakes can be high: a filter that is too permissive will cause the user’s inbox to get filled with junk mail, while a filter that is overly restrictive could cause relevant messages to be directed to the spam folder. In either case, the email user could miss important messages.</p>
<p>In the taxonomic assignment example that we’ll work through in this chapter, our classes will be microbial species. Our species classifier for 16S rRNA sequences will take an unannotated sequence as input and as output present the species that the sequence most likely originated from.</p>
</div>
<div class="section" id="training-data-test-data-and-cross-validation">
<h3>Training data, test data, and cross-validation<a class="headerlink" href="#training-data-test-data-and-cross-validation" title="Permalink to this headline">¶</a></h3>
<p>Supervised classification algorithms need to be provided with data that is used to develop a model for use in classification. Developing this model is referred to as training the classifier. The data that is used for this is a collection of observations with defined classes, and is referred to as the <strong>training data</strong>. These labeled examples are the “supervision” aspect of supervised learning. In the email spam filter example, this would be email messages that are annotated as either spam or not spam. In the species assignment example, this would be 16S sequences that are taxonomically annotated at the species level. It is typically important that the training data be balanced - in other words, that there are roughly the same number of examples of each class.</p>
<p>In addition to the training data, an independent collection of observations with defined classes is needed as <strong>test data</strong>. These observations are not used to train the classifier, but rather to evaluate how the classifier performs on previously unseen data. The goal of testing the classifier on these test data is to predict what performance will be on <strong>real world</strong> data. Real world data refers to data for which the class is currently unknown. In the spam filter example, real world data would be new emails that you are receiving. In the species assignment example, real world data could be sequences that you obtain from the environment using a DNA sequencing instrument. The test data shouldn’t be used for optimization of classifiers: in other words, you shouldn’t develop a classifier on training data, test it on test data, go back and make changes to the classifier, and then re-test on test data. This would risk <strong>over-fitting</strong> the classifier to a particular test data set and performance on that test data may no longer be predictive of how the classifier will perform when it is used on real world data.</p>
<p>Because training and test data sets can be very costly to develop (for example, they may require many hours of annotation by humans) we often use an approach called <strong>k-fold cross validation</strong> during classifier development and optimization <a class="reference internal" href="#cross-validation-1"><span class="std std-numref">Fig. 7</span></a>. In k-fold cross-validation, the training data is split into <code class="docutils literal notranslate"><span class="pre">k</span></code> different data sets, where <code class="docutils literal notranslate"><span class="pre">k</span></code> is usually five or ten. In each of the data sets, <span class="math notranslate nohighlight">\(1/k\)</span> of the entries are used as test data and all of the other entries are used as training data. In <code class="docutils literal notranslate"><span class="pre">k</span></code> iterations, the classifier is developed on the training data and tested on the test data. The average performance of the classifier is then computed across the <code class="docutils literal notranslate"><span class="pre">k</span></code> iterations. k-fold cross validation therefore allows for developing and optimizing a classifier without using dedicated test data.</p>
<div class="figure align-default" id="cross-validation-1">
<img alt="_images/ml-cross-validation.png" src="_images/ml-cross-validation.png" />
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">An illustration of k-fold cross validation where a single data set is split into k independent training and test data sets. Each circle represents a labeled entry for use in training or testing, and colors indicate the class of each entry. In the case of a spam filter, for example, red circles might represent spam messages while green circles represent messages that are not spam.
Image source: <a class="reference external" href="https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg">Gufosowa</a>, <a class="reference external" href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons.</span><a class="headerlink" href="#cross-validation-1" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="evaluating-a-binary-classifier">
<h3>Evaluating a binary classifier<a class="headerlink" href="#evaluating-a-binary-classifier" title="Permalink to this headline">¶</a></h3>
<p>As mentioned above, in a classification task there are two or more pre-defined classes. A binary classifier would be a specific type of classifier for which there are exactly two classes - for example, spam and not spam. We’ll start talking about how classifiers are evaluated by discussing binary classifiers because they’re the easiest to understand.</p>
<p>Imagine we’re building a classifier that attempts to predict whether an individual is healthy or has some specific disease (let’s call it <em>Disease X</em>). Perhaps the data that the classifier uses is based on a variety of medical data that has undergone a feature extraction process to generate features that can be used by a supervised classification algorithm. When a classifier is developed, you can think of it like a function that will take a collection of features for a sample and return a value of “healthy” or “diseased”.</p>
<p>The goal of our classifier is to serve as a diagnostic tool that identifies whether a patient has Disease X based on features of their medical data. A positive test result therefore indicates that the patient has Disease X while a negative test result indicates that they are healthy. When we apply our classifier to test data (i.e., where we know the correct class), there are a few possible outcomes.</p>
<ul class="simple">
<li><p>The classifier predicts a positive test result, and the sample is known to come from a patient with Disease X. This is a <strong>true positive (TP)</strong>.</p></li>
<li><p>The classifier predicts a positive test result, and the sample is known to come from a healthy patient. This is a <strong>false positive (FP)</strong>. FPs are also referred to as type 1 errors.</p></li>
<li><p>The classifier predicts a negative test result, and the sample is known to come from a patient with Disease X. This is a <strong>false negative (FN)</strong>. FNs are also referred to as type 2 errors.</p></li>
<li><p>The classifier predicts a negative test result, and the sample is known to come from a healthy patient. This is a <strong>true negative (TN)</strong>.</p></li>
</ul>
<p>A classifier would typically be evaluated by running it on many samples and tallying the count of TP, FP, FN, and TN results. These tallies are typically presented in a structure known as a <strong>confusion matrix</strong>. For the confusion matrix, there many different values that can be computed which inform us of some aspect of classifier performance.</p>
<p>The simplest way to think about evaluating the performance of our classifier from a confusion matrix is to compute its <strong>accuracy</strong> as:</p>
<div class="math notranslate nohighlight" id="equation-accuracy">
<span class="eqno">(5)<a class="headerlink" href="#equation-accuracy" title="Permalink to this equation">¶</a></span>\[accuracy = \frac{TP + TN}{TP + FP + FN + TN}\]</div>
<p>In words, accuracy can be defined as the fraction of the total test cases that the classifier classified correctly. Accuracy gives us an idea of the classifier performance, but it hides some potentially relevant information from us. Specifically, it doesn’t tell us whether poor classifier performance is a result of primarily Type 1 errors, primarily Type 2 errors, or a balance of the two. A low accuracy classifier could, for example, frequently return false positives (Type 1 errors) but almost never return false negatives (Type 2 errors). Such a classifier could still be a clinically useful tool. Because false negatives are very infrequent but false positives are common, that means when the classifier indicates a negative test result that person doesn’t have the disease. If the classifier indicates a positive result, that could be an indicator that additional testing is needed. Of course we would rather our classifier achieve fewer false positives, but if this is a very cheap test and the additional tests are more expensive, it can be a useful first screening approach.</p>
<p>Two other metrics are more widely used for evaluating classifiers, and these are typically computed as a pair. These metrics are <strong>precision</strong> and <strong>recall</strong> and they are more informative than accuracy because they indicate whether a classifier might suffer more from false positives or false negatives.</p>
<p>Precision is the fraction of the positives reported by the classifier that are actually positives, or:</p>
<div class="math notranslate nohighlight" id="equation-precision">
<span class="eqno">(6)<a class="headerlink" href="#equation-precision" title="Permalink to this equation">¶</a></span>\[precision = \frac{TP}{TP + FP}\]</div>
<p>Recall is the fraction of the actual positives that are reported to be positive by the classifier, or:</p>
<div class="math notranslate nohighlight" id="equation-recall">
<span class="eqno">(7)<a class="headerlink" href="#equation-recall" title="Permalink to this equation">¶</a></span>\[recall = \frac{TP}{TP + FN}\]</div>
<p>Precision thus tells us how frequently our classifier yields false positives, while recall tells us how frequently our classifier yields false negatives. We of course would always like both of these values to be high, but depending on the application of our classifier, we may prefer high precision over high recall, or we may prefer high recall over high precision.</p>
</div>
<div class="section" id="naive-bayes-classifiers">
<h3>Naive Bayes classifiers<a class="headerlink" href="#naive-bayes-classifiers" title="Permalink to this headline">¶</a></h3>
<p>In this chapter, instead of using sequence alignment to identify the most likely taxonomic origin of a sequence, we’ll train Naive Bayes classifiers to do this by building <a class="reference internal" href="database-searching.html#kmer"><span class="std std-ref">kmer</span></a>-based models of the 16S sequences of taxa in our training data. We’ll then run test sequences through those models to identify the most likely taxonomic origin of each test sequence. Since we know the taxonomic origin of our test sequences, we can evaluate the accuracy of our classifiers by seeing how often they return the known taxonomy assignment. If our training and testing approaches are well-designed, the performance on our tests will inform us of how accurate we can expect our classifier to be on data where the actual taxonomic origin is unknown.</p>
</div>
<div class="section" id="training-a-native-bayes-classifier">
<h3>Training a Native Bayes classifier<a class="headerlink" href="#training-a-native-bayes-classifier" title="Permalink to this headline">¶</a></h3>
<p>Naive Bayes classifiers work by building a model of what different classes look like based on labeled training data. As with unsupervised learning tasks, the starting point is a feature table representing instances of the different classes. In addition to the feature table, since this is a supervised learning task, the sequence labels (i.e., the class labels) will also be used to train the classifier.</p>
<p>We’ll again use k-mers as our features, and continue with the value of <code class="docutils literal notranslate"><span class="pre">k</span></code> that we defined above. The first thing our Naive Bayes classifier will need is the set of all possible features, which in our case will be all possible words of length <code class="docutils literal notranslate"><span class="pre">k</span></code>. This will be dependent on the value of <code class="docutils literal notranslate"><span class="pre">k</span></code> and the characters in our alphabet (i.e., the characters that we should expect to find in the sequences in our reference database). This set is referred to as <code class="docutils literal notranslate"><span class="pre">W</span></code>, and can be computed as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphabet</span> <span class="o">=</span> <span class="n">skbio</span><span class="o">.</span><span class="n">DNA</span><span class="o">.</span><span class="n">nondegenerate_chars</span>

<span class="k">def</span> <span class="nf">compute_W</span><span class="p">(</span><span class="n">alphabet</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">,</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">alphabet</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">k</span><span class="p">)))</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">compute_W</span><span class="p">(</span><span class="n">alphabet</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Alphabet contains the characters: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">alphabet</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For an alphabet size of </span><span class="si">%d</span><span class="s1">, W contains </span><span class="si">%d</span><span class="s1"> length-</span><span class="si">%d</span><span class="s1"> kmers.&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alphabet</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="n">k</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Alphabet contains the characters: G, T, A, C
For an alphabet size of 4, W contains 256 length-4 kmers.
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Given the DNA alphabet (A, C, G, and T), how many different kmers of length 3 are there (i.e., 3-mers)? How many different 5-mers are there? How many 5-mers are there if there are twenty characters in our alphabet (as would be the case if we were working with protein sequences instead of DNA sequences)?</p>
</div>
<p>To train our taxonomic classifier, we also need to know what level of taxonomic specificity we want to classify our sequences to. We should expect to achieve higher accuracy at less specific taxonomic levels such as phylum or class, but these are likely to be less informative biologically than more specific levels such as genus or species. Again, we’ll mirror the choice we made for our unsupervised learning task and attempt to build a species classifier here.</p>
<p>With this information, we can next compute our <strong>k-mer probability table</strong>. The goal for this table is that it accurately represents the probability of observing each k-mer in <code class="docutils literal notranslate"><span class="pre">W</span></code> in a sequence from a given species. Because we don’t know these probabilities, we estimate them based on the frequency that we observe each k-mer in the sequences in our training data. Our k-mer probability table is computed using the following values:</p>
<p><span class="math notranslate nohighlight">\(N\)</span> : The total number of sequences in the training data.</p>
<p><span class="math notranslate nohighlight">\(W\)</span>: The set of all possible kmers, given <span class="math notranslate nohighlight">\(k\)</span> and an alphabet.</p>
<p><span class="math notranslate nohighlight">\(w_i\)</span>: An individual k-mer in <span class="math notranslate nohighlight">\(W\)</span>.</p>
<p><span class="math notranslate nohighlight">\(n(w_i)\)</span> : The total number of training data sequences containing <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<p><span class="math notranslate nohighlight">\(P_i\)</span> : The probability of observing <span class="math notranslate nohighlight">\(w_i\)</span> in a relevant real world sequence from any species. Initially it might seem as though this would be computed as <span class="math notranslate nohighlight">\(n(w_i) / N\)</span>, but this neglects the possibility that a k-mer in a real world sequence might not be represented in any sequences in our training data (i.e., <span class="math notranslate nohighlight">\(n(w_i) = 0\)</span>). This would cause a problem when classifing that real world sequence - we’ll revisit this shortly. As a result, 0.5 is added to the numerator and 1 is added to the denominator so that this is computed as <span class="math notranslate nohighlight">\((n(w_i) + 0.5) / (N + 1)\)</span>. When we add to our counts in this way, we refer to the values that we’re adding as <strong>pseudocounts</strong>.</p>
<p><span class="math notranslate nohighlight">\(P(w_i | species)\)</span> : The probability of observing <span class="math notranslate nohighlight">\(w_i\)</span> in a relevant real world sequence from a given species. Again, it would seem that this would be computed as the proportion of sequences in the species containing <span class="math notranslate nohighlight">\(w_i\)</span>, but this would neglect that we’re likely to observe k-mers in real-world sequences that are not represented in our training data. A pseudocount is therefore added again to the numerator and denominator. This time the pseudocount in the numerator is scaled by how frequent <span class="math notranslate nohighlight">\(w_i\)</span> is in the reference database as a whole: specifically, it is <span class="math notranslate nohighlight">\(P_i\)</span>. The pseudocount in the denominator is still 1.</p>
<p>Our “kmer probability table” is <span class="math notranslate nohighlight">\(P(w_i | species)\)</span> computed for all kmers in W and all species represented in our reference database. Let’s compute that, and then look at the first 25 rows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_kmer_probability_table</span><span class="p">(</span><span class="n">feature_table</span><span class="p">,</span> <span class="n">sequence_labels</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">feature_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of training sequences</span>

    <span class="c1"># number of sequences containing kmer wi</span>
    <span class="n">n_wi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">W</span><span class="p">)</span>
    <span class="n">n_wi</span> <span class="o">=</span> <span class="n">n_wi</span> <span class="o">+</span> <span class="n">feature_table</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">n_wi</span> <span class="o">=</span> <span class="n">n_wi</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">n_wi</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;n(w_i)&#39;</span>

    <span class="c1"># probabilities of observing each kmer</span>
    <span class="n">Pi</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_wi</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">Pi</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;P_i&#39;</span>
    
    <span class="c1"># number of times each taxon appears in training set</span>
    <span class="n">taxon_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">sequence_labels</span><span class="p">)</span>
    <span class="n">taxon_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">taxon_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">W</span><span class="p">)</span>
    <span class="n">taxon_table</span> <span class="o">=</span> <span class="n">taxon_table</span> <span class="o">+</span> <span class="n">feature_table</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">sequence_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">taxon_table</span> <span class="o">=</span> <span class="n">taxon_table</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># probabilities of observing each kmer in each taxon</span>
    <span class="n">p_wi_t</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">taxon</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">taxon_counts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">p_wi_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">((</span><span class="n">taxon_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">taxon</span><span class="p">]</span> <span class="o">+</span> <span class="n">Pi</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">count</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">taxon</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">p_wi_t</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kmer_probability_table</span> <span class="o">=</span> <span class="n">compute_kmer_probability_table</span><span class="p">(</span><span class="n">sequence_feature_table</span><span class="p">,</span> <span class="n">sequence_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">],</span> <span class="n">W</span><span class="p">)</span>
<span class="n">kmer_probability_table</span><span class="p">[:</span><span class="mi">25</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pseudomonas viridiflava (Proteobacteria)</th>
      <th>Propionibacterium acnes (Actinobacteria)</th>
      <th>Prevotella copri (Bacteroidetes)</th>
      <th>Pseudomonas veronii (Proteobacteria)</th>
      <th>Flavobacterium succinicans (Bacteroidetes)</th>
      <th>Prevotella melaninogenica (Bacteroidetes)</th>
      <th>Prevotella stercorea (Bacteroidetes)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AAAA</th>
      <td>0.201389</td>
      <td>0.201389</td>
      <td>0.368056</td>
      <td>0.034722</td>
      <td>0.201389</td>
      <td>0.368056</td>
      <td>0.034722</td>
    </tr>
    <tr>
      <th>AAAC</th>
      <td>0.655093</td>
      <td>0.988426</td>
      <td>0.988426</td>
      <td>0.988426</td>
      <td>0.988426</td>
      <td>0.988426</td>
      <td>0.988426</td>
    </tr>
    <tr>
      <th>AAAG</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AAAT</th>
      <td>0.224537</td>
      <td>0.224537</td>
      <td>0.224537</td>
      <td>0.057870</td>
      <td>0.891204</td>
      <td>0.057870</td>
      <td>0.724537</td>
    </tr>
    <tr>
      <th>AACA</th>
      <td>0.636574</td>
      <td>0.969907</td>
      <td>0.969907</td>
      <td>0.469907</td>
      <td>0.969907</td>
      <td>0.803241</td>
      <td>0.969907</td>
    </tr>
    <tr>
      <th>AACC</th>
      <td>0.460648</td>
      <td>0.960648</td>
      <td>0.960648</td>
      <td>0.293981</td>
      <td>0.793981</td>
      <td>0.960648</td>
      <td>0.960648</td>
    </tr>
    <tr>
      <th>AACG</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AACT</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AAGA</th>
      <td>0.798611</td>
      <td>0.965278</td>
      <td>0.798611</td>
      <td>0.798611</td>
      <td>0.965278</td>
      <td>0.798611</td>
      <td>0.465278</td>
    </tr>
    <tr>
      <th>AAGC</th>
      <td>0.956019</td>
      <td>0.956019</td>
      <td>0.956019</td>
      <td>0.956019</td>
      <td>0.622685</td>
      <td>0.289352</td>
      <td>0.456019</td>
    </tr>
    <tr>
      <th>AAGG</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AAGT</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AATA</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AATC</th>
      <td>0.923611</td>
      <td>0.256944</td>
      <td>0.090278</td>
      <td>0.923611</td>
      <td>0.923611</td>
      <td>0.423611</td>
      <td>0.256944</td>
    </tr>
    <tr>
      <th>AATG</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>AATT</th>
      <td>0.747685</td>
      <td>0.081019</td>
      <td>0.414352</td>
      <td>0.247685</td>
      <td>0.914352</td>
      <td>0.581019</td>
      <td>0.414352</td>
    </tr>
    <tr>
      <th>ACAA</th>
      <td>0.942130</td>
      <td>0.942130</td>
      <td>0.442130</td>
      <td>0.775463</td>
      <td>0.942130</td>
      <td>0.275463</td>
      <td>0.275463</td>
    </tr>
    <tr>
      <th>ACAC</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>ACAG</th>
      <td>0.969907</td>
      <td>0.303241</td>
      <td>0.969907</td>
      <td>0.803241</td>
      <td>0.969907</td>
      <td>0.803241</td>
      <td>0.969907</td>
    </tr>
    <tr>
      <th>ACAT</th>
      <td>0.131944</td>
      <td>0.965278</td>
      <td>0.965278</td>
      <td>0.798611</td>
      <td>0.798611</td>
      <td>0.965278</td>
      <td>0.965278</td>
    </tr>
    <tr>
      <th>ACCA</th>
      <td>0.826389</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
    </tr>
    <tr>
      <th>ACCC</th>
      <td>0.428241</td>
      <td>0.428241</td>
      <td>0.928241</td>
      <td>0.428241</td>
      <td>0.261574</td>
      <td>0.594907</td>
      <td>0.928241</td>
    </tr>
    <tr>
      <th>ACCG</th>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
      <td>0.997685</td>
    </tr>
    <tr>
      <th>ACCT</th>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.993056</td>
      <td>0.826389</td>
      <td>0.993056</td>
    </tr>
    <tr>
      <th>ACGA</th>
      <td>0.645833</td>
      <td>0.812500</td>
      <td>0.812500</td>
      <td>0.979167</td>
      <td>0.979167</td>
      <td>0.979167</td>
      <td>0.979167</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This k-mer probability table represents our k-mer-based models of the species in our training data. We can use this table to compute probabilities of real world sequences belonging to each of the species represented in this table.</p>
</div>
<div class="section" id="applying-a-naive-bayes-classifier">
<h3>Applying a Naive Bayes classifier<a class="headerlink" href="#applying-a-naive-bayes-classifier" title="Permalink to this headline">¶</a></h3>
<p>With our k-mer probability table we are now ready to classify unknown sequences. We’ll begin by selecting sequences that will serve as our test data. We’ll pull sequences for our species of interest at random from our reference database, excluding sequences that were used in our training data.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_seq_data</span> <span class="o">=</span> <span class="n">load_annotated_sequences</span><span class="p">(</span><span class="n">taxa_of_interest</span><span class="p">,</span> <span class="n">class_size</span><span class="o">=</span><span class="n">sequences_per_taxon</span><span class="p">,</span> 
                                          <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ids_to_exclude</span><span class="o">=</span><span class="n">sequence_labels</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">feature_labels_from_sequence_records</span><span class="p">(</span><span class="n">test_seq_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now review a few of the sequences that were selected for our test data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">sr</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">test_seq_data</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sr</span><span class="o">.</span><span class="n">taxonomy</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sr</span><span class="o">.</span><span class="n">sequence</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;🦠&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4349788
k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__viridiflava
AGTGGAGCGCGAGGAGAAGAGCTTGCTCTTCGATTCATCGGCTGACGGGTGACTAATGCCTAGGAATCTGCCTGGTGGTGGGGGAGAACGTTTCTAAAGGAACGCTAATACCGCATACGTCCTACGGGAGAAAGCAGGGGACCTTCGGGCCTTGCGCTATCAGATGAGCCTAGGTCGGATTAGCTAGTTGGTGAGGTAATGGCTCACCAAGGCGACGATCCGTAACTGGTCTGAGAGGATGATCAGTCACACTGGAACTGAGACACGGTCCAGACTCCTACGGGAGGCAGCAGTGGGGAATATTGGACAATGGGCGAAAGCCTGATCCAGCCATGCCGCGTGTGTGAAGAAGGTCTTCGGATTGTAAAGCACTTTAAGTTGGGAGGAAGGGCAGTAAGCTAATACCTTGCTGTTTTGACGTTACCAACAGAATAAGCACCGGCTAACTCTGTGCCCAGCAGCCGCGGTAATACAGAGGGTGCAAGCGTTAATCGGAATTA
🦠
130002
k__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Pseudomonadales;f__Pseudomonadaceae;g__Pseudomonas;s__viridiflava
ATTGAACGCTGGCGGCAGGCCTAACACATGCAAGTCGAGCGGATGAGGAGAGCTTGCTCTTCGATTCAGCGGCGGACGGGTGAGTAATGCCTAGGAATCTGCCTGGTAGTGGGGGACAACGTTTCGAAAGGGACGCTAATACCGCATACGTCCTACGGGAGAAAGCAGGGGACCTTCGGGCCTTGCGCTATCAGATGAGCCTAGGTCGGATTAGCTAGTTGGTGAGGTAATGGCTCACCAAGGCGACGATCCGTAACTGGTCTGAGAGGATGATCAGTCACACTGGAACTGAGACACGGTCCAGACTCCTACGGGAGGCAGCAGTGGGGAATATTGGACAATGGGCGAAAGCCTGATCCAGCCATGCCGCGTGTGTGAAGAAGGTCTTCGGATTGTAAAGCACTTTAAGTTGGGAGGAAGGGCAGTAAGCTAATACCTTACTGTTTTGACGTTACCGACAGAATAAGCACCGGCTAACTCTGTGCCAGCAGCCGCGGTAA
🦠
1066621
k__Bacteria;p__Bacteroidetes;c__Bacteroidia;o__Bacteroidales;f__Prevotellaceae;g__Prevotella;s__melaninogenica
GATGAACGCTAGCTACAGCGCTTAACACATGCAAGTCGAGGGGAAACGGCATTGAGTGCTTGCACTCTTTGGACGTCGACCGGCACACGGGTGAGTAACGCGTATCCAACCTTCCCATTACTGTGGGATAACCTGCCGAAAGGCAGACTAATACCGCATAGTCTTCGATGACGGCCTCAGATTTGAAGTAAAGATTTATCGGTAATGGATGGGGATGCGTCTGATTAGCTTGTTGGCGGGGTAACGGCCCACCAAGGCGACGATCAGTAGGGGTTCTGAGAGGAAGGTCCCCCACATTGGAACTGAGACACGGTCCAAACTCCTACGGGAGGCAGCAGTGAAGAATATTGGTCAATGGACGGAAGTCTGAACCAGCCAAGTAGCGTGCAGGATGACGGCCCTATGGGTTGTAAACTGCTTTTGTATGGGGATAAAGTTAGGGACGTGTCCCTATTTGCAGGTACCATACTAATAAGGACCGGCTAATTCCCTGCCAGCAT
🦠
</pre></div>
</div>
</div>
</div>
<p>For a sequence that is provided as input to our Naive Bayes classifier, which is generally referred to as a query sequence, taxonomy will be assigned as follows. First, the set of all k-mers will be extracted from the query sequence. This set of k-mers is referred to as <span class="math notranslate nohighlight">\(V\)</span>. Then, for each species represented in the k-mer probability table, the probability of observing the sequence will be computed assuming that the sequence is a representative of that species. This is referred to as the probability of the query sequence given the species, or <span class="math notranslate nohighlight">\(P(query | species)\)</span>. This is computed as the product of all its k-mer probabilities for the given species. It should be clear based on this formula why it was necessary to add pseudocounts when computing our k-mer probability table. If not, k-mer probabilities of zero would result in a zero probability of the sequence being derived from that taxon at this step.</p>
<p>After computing <span class="math notranslate nohighlight">\(P(query | species)\)</span> for each species, the taxonomy assignment returned is simply the one achieving the highest probability. Here we’ll classify a sequence and look at the resulting taxonomy assignment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This function classifies a sequence that has already been split into a list</span>
<span class="c1"># of kmers.</span>
<span class="k">def</span> <span class="nf">classify_V</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">):</span>
    <span class="n">P_S_t</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># probability of the sequence given the taxon</span>
    <span class="k">for</span> <span class="n">taxon</span> <span class="ow">in</span> <span class="n">kmer_probability_table</span><span class="p">:</span>
        <span class="n">kmer_probabilities</span> <span class="o">=</span> <span class="n">kmer_probability_table</span><span class="p">[</span><span class="n">taxon</span><span class="p">]</span>
        <span class="c1"># TODO: Confirm this step</span>
        <span class="c1"># Because we&#39;re multiplying many probabilities, we often will hit the lower</span>
        <span class="c1"># limit of the computer&#39;s precision (i.e., our probability will be </span>
        <span class="c1"># less than machine epsilon). We therefore take the log of each observed </span>
        <span class="c1"># probability and sum those logs. </span>
        <span class="n">query_log_probability</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">,</span> <span class="n">kmer_probabilities</span><span class="p">[</span><span class="n">V</span><span class="p">])))</span>
        <span class="n">P_S_t</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">query_log_probability</span><span class="p">,</span> <span class="n">taxon</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">P_S_t</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">V</span>

<span class="c1"># This function is more convenient to use. It classifies a sequence </span>
<span class="c1"># directly, first by computing V, and then by calling classify_V.</span>
<span class="k">def</span> <span class="nf">classify_sequence</span><span class="p">(</span><span class="n">query_sequence</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">query_sequence</span><span class="o">.</span><span class="n">iter_kmers</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">classify_V</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now apply the Naive Bayes classifier to sequences in our test data set. Here I select a single test sequence, and then provide that as input to the <code class="docutils literal notranslate"><span class="pre">classify_sequence</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_sequence_record_choice</span><span class="p">(</span><span class="n">sequence_records</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">sequence_records</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">query_sr</span> <span class="o">=</span> <span class="n">random_sequence_record_choice</span><span class="p">(</span><span class="n">test_seq_data</span><span class="p">)</span>
<span class="n">taxon_assignment</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">classify_sequence</span><span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">sequence</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence </span><span class="si">%s</span><span class="s2"> is predicted to be from the species </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">taxon_assignment</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequence 517068 is predicted to be from the species Prevotella copri (Bacteroidetes).
</pre></div>
</div>
</div>
</div>
<p>Because this query sequence is from our test data, we know the actual taxonomy assignment as we can look it up to determine if our classifier was correct. This is in contrast to applying our classifier to real world query sequences, where we typically won’t know what the correct assignment in. We can use our test data to estimate how well we expect our classifier to perform on real world data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence </span><span class="si">%s</span><span class="s2"> is known to be from the species </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">][</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequence 517068 is known to be from the species Prevotella copri (Bacteroidetes).
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Was this sequence classified as the correct species? If not, was it at least classified to the correct genus? What about the correct phylum?</p>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Try classifying a few other query sequences and determining if the returned species assignment was correct. You can do this by running the previous two code cells over again in order. Does this classifier seem to be working well?</p>
</div>
</div>
<div class="section" id="evaluating-our-confidence-in-the-results-of-the-naive-bayes-classifier">
<h3>Evaluating our confidence in the results of the Naive Bayes classifier<a class="headerlink" href="#evaluating-our-confidence-in-the-results-of-the-naive-bayes-classifier" title="Permalink to this headline">¶</a></h3>
<p>Because the training and test sequences that we’re working with were randomly selected from the full reference database, each time you run this notebook you should observe different results. If you run the above steps multiple times you’ll get the wrong taxonomy assignment at least some of the time, most likely. Up to this point, we’ve left out an important piece of information: how confident should we be in our assignment? In other words, how dependent is our taxonomy assignment on our specific query sequence? If there were slight differences in our query sequence (e.g., because we observed a very closely related organism, such as one of the same species but a different strain, or because there are some sequencing errors in our sequence) would we obtain the same taxonomy assignment? If so, we should have higher confidence in our assignment. If not, we should have lower confidence in our assignment. This is important because our classifier as implemented so far will <em>always</em> return one of the classes, even if our query sequence is very different than any of the sequences in our reference database.</p>
<p>We can quantify confidence in our taxonomic assignments using an approach called bootstrapping. With a bootstrap approach, we’ll get our taxonomy assignment as we did above, but then for some user-specified number of iterations, we’ll create random subsets of <span class="math notranslate nohighlight">\(V\)</span> sampled with replacement. We’ll then assign taxonomy to each random subset of <span class="math notranslate nohighlight">\(V\)</span>, and count the number of times the resulting taxonomy assignment is the same as the one we received when assigning taxonomy to <span class="math notranslate nohighlight">\(V\)</span>. The count of times that they are the same divided by the number of iterations we’ve chosen to run will be our confidence in the assignment. If the assignments are often the same we’ll have a high confidence value, up to a maximum confidence value of 1 if the assignments are always the same. If the assignments are often different we’ll have a low confidence value, down to a minimum value of 0 if the assignments are never the same.</p>
<p>The following funtion will assign taxonomy to a query sequence, and will compute and return a confidence value for the assignment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">classify_sequence_with_confidence</span><span class="p">(</span><span class="n">query_sequence</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span>
                                      <span class="n">confidence_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># classify the query sequence, as we did above</span>
    <span class="n">taxon</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">classify_sequence</span><span class="p">(</span><span class="n">query_sequence</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="n">count_same_taxon</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Define the size of each subsample as 10% of the actual number of</span>
    <span class="c1"># kmers in the query sequence.</span>
    <span class="n">subsample_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># Perform n iterations (where n is provided by the user as </span>
    <span class="c1"># confidence_iterations) where a random subset of the query sequence&#39;s</span>
    <span class="c1"># kmers are used for the classification task.</span>
    <span class="c1"># Keep track of the number of times the observed result is the same as</span>
    <span class="c1"># that for the query sequence. </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">confidence_iterations</span><span class="p">):</span>
        <span class="n">subsample_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">subsample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">subsample_taxon</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">classify_V</span><span class="p">(</span><span class="n">subsample_V</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">taxon</span> <span class="o">==</span> <span class="n">subsample_taxon</span><span class="p">:</span>
            <span class="n">count_same_taxon</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">confidence</span> <span class="o">=</span> <span class="n">count_same_taxon</span> <span class="o">/</span> <span class="n">confidence_iterations</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">taxon</span><span class="p">,</span> <span class="n">confidence</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can apply this to a randomly selected sequence from our test data as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query_sr</span> <span class="o">=</span> <span class="n">random_sequence_record_choice</span><span class="p">(</span><span class="n">test_seq_data</span><span class="p">)</span>
<span class="n">taxon_assignment</span><span class="p">,</span> <span class="n">confidence</span> <span class="o">=</span> <span class="n">classify_sequence_with_confidence</span><span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">sequence</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">taxon_assignment</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence </span><span class="si">%s</span><span class="s2"> is predicted to be from the species </span><span class="si">%s</span><span class="s2">. Confidence in this assignment is: </span><span class="si">%1.2f</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">taxon_assignment</span><span class="p">,</span> <span class="n">confidence</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pseudomonas viridiflava (Proteobacteria)
0.55
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequence 454228 is predicted to be from the species Pseudomonas viridiflava (Proteobacteria). Confidence in this assignment is: 0.55.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence </span><span class="si">%s</span><span class="s2"> is known to be from the species </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">][</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequence 454228 is known to be from the species Pseudomonas viridiflava (Proteobacteria).
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Was this sequence classified as the correct species? Does the confidence value align with this result?</p>
</div>
<p>At first glance, we don’t necessarily have an idea of what good versus bad confidence scores are, but we can use our test data to explore that. Once we know what a good confidence score is, we can apply a confidence threshold that we can use in our work. For example, we can define a confidence threshold above which we would accept a taxonomy assignment and below which we could label a sequence as “unclassified”. To explore this, let’s compute taxonomy assignments and confidence for all of our test sequences and then see what the distributions of confidence scores look like for correct assignments and incorrect assignments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">correct_assignment_confidences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">incorrect_assignment_confidences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">summary</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">query_id</span><span class="p">,</span> <span class="n">query_sr</span> <span class="ow">in</span> <span class="n">test_seq_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">predicted_taxonomy</span><span class="p">,</span> <span class="n">confidence</span> <span class="o">=</span> <span class="n">classify_sequence_with_confidence</span><span class="p">(</span><span class="n">query_sr</span><span class="o">.</span><span class="n">sequence</span><span class="p">,</span> <span class="n">kmer_probability_table</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">known_taxonomy</span> <span class="o">=</span> <span class="n">test_labels</span><span class="p">[</span><span class="s1">&#39;legend entry&#39;</span><span class="p">][</span><span class="n">query_sr</span><span class="o">.</span><span class="n">identifier</span><span class="p">]</span>
    <span class="n">correct_assignment</span> <span class="o">=</span> <span class="n">known_taxonomy</span> <span class="o">==</span> <span class="n">predicted_taxonomy</span>
    <span class="k">if</span> <span class="n">correct_assignment</span><span class="p">:</span>
        <span class="n">correct_assignment_confidences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">incorrect_assignment_confidences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">confidence</span><span class="p">)</span>

    <span class="n">summary</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">predicted_taxonomy</span><span class="p">,</span> <span class="n">known_taxonomy</span><span class="p">,</span> <span class="n">confidence</span><span class="p">,</span> <span class="n">correct_assignment</span><span class="p">])</span>

<span class="n">summary</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">summary</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Predicted taxonomy&#39;</span><span class="p">,</span> <span class="s1">&#39;Known taxonomy&#39;</span><span class="p">,</span> <span class="s1">&#39;Confidence&#39;</span><span class="p">,</span> <span class="s1">&#39;Correct assignment&#39;</span><span class="p">])</span>

<span class="n">summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted taxonomy</th>
      <th>Known taxonomy</th>
      <th>Confidence</th>
      <th>Correct assignment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>0.60</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>0.87</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>0.63</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>0.53</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>Pseudomonas viridiflava (Proteobacteria)</td>
      <td>0.68</td>
      <td>True</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>0.55</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>0.65</td>
      <td>True</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>0.83</td>
      <td>True</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>0.83</td>
      <td>True</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>Propionibacterium acnes (Actinobacteria)</td>
      <td>0.88</td>
      <td>True</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>0.64</td>
      <td>True</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>0.52</td>
      <td>True</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>0.71</td>
      <td>True</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>0.71</td>
      <td>True</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>0.70</td>
      <td>True</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>0.64</td>
      <td>True</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>0.75</td>
      <td>True</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>0.77</td>
      <td>True</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>0.81</td>
      <td>True</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>Pseudomonas veronii (Proteobacteria)</td>
      <td>0.59</td>
      <td>True</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>0.74</td>
      <td>True</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>0.71</td>
      <td>True</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>0.72</td>
      <td>True</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>0.67</td>
      <td>True</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>Flavobacterium succinicans (Bacteroidetes)</td>
      <td>0.96</td>
      <td>True</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>0.64</td>
      <td>True</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>0.84</td>
      <td>True</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>0.78</td>
      <td>True</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>0.58</td>
      <td>True</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>0.69</td>
      <td>True</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>0.38</td>
      <td>False</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>0.69</td>
      <td>True</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Prevotella copri (Bacteroidetes)</td>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>0.26</td>
      <td>False</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>0.64</td>
      <td>True</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Prevotella melaninogenica (Bacteroidetes)</td>
      <td>Prevotella stercorea (Bacteroidetes)</td>
      <td>0.42</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Comparing distributions of confidence scores for correct and incorrect assignments is possible from the table above, and the table provides details that can be useful in assessing when the classifier is working well and when it isn’t. A couple of boxplots however will make comparing these distributions trivial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">correct_assignment_confidences</span><span class="p">,</span> <span class="n">incorrect_assignment_confidences</span><span class="p">])</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">swarmplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">correct_assignment_confidences</span><span class="p">,</span> <span class="n">incorrect_assignment_confidences</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;Correct assignments&#39;</span><span class="p">,</span> <span class="s1">&#39;Incorrect assignments&#39;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Confidence&#39;</span><span class="p">)</span>

<span class="n">ax</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:ylabel=&#39;Confidence&#39;&gt;
</pre></div>
</div>
<img alt="_images/machine-learning_97_1.png" src="_images/machine-learning_97_1.png" />
</div>
</div>
<p>What does this plot tell you about how well setting a confidence threshold is likely to work? If you never wanted to reject a correct assignment, how often would you accept an incorrect assignment? If you never wanted to accept an incorrect assignment, how often would you reject a correct assignment?</p>
<p>We can also compute the overall accuracy of our classifier as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_correct_assignments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">correct_assignment_confidences</span><span class="p">)</span>
<span class="n">n_incorrect_assignments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">incorrect_assignment_confidences</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">n_correct_assignments</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_correct_assignments</span> <span class="o">+</span> <span class="n">n_incorrect_assignments</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The accuracy of the classifier was </span><span class="si">%1.3f</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy of the classifier was 0.886.
</pre></div>
</div>
</div>
</div>
<p>Finally, we can summarize how this classifier worked by creating a <strong>confusion matrix</strong>. A confusion matrix has two axes - one corresponding to the actual assignments and one corresponding to the predicted assignments. The values in the confusion matrix represent how each instance of each known taxonomy was classified. The order of the classes on the two axes should always be the same in a confusion matrix. A good classifier will then have high values along the diagonal. If classifier accuracy is not great, you can see which known classes were misassigned, or where the classifier got confused.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">summary</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Predicted taxonomy&#39;</span><span class="p">)[</span><span class="s1">&#39;Known taxonomy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">confusion_matrix</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;count&#39;</span><span class="p">]</span>
<span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">pivot_table</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Predicted taxonomy&#39;</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Known taxonomy&#39;</span><span class="p">],</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">confusion_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="7" halign="left">count</th>
    </tr>
    <tr>
      <th>Known taxonomy</th>
      <th>Flavobacterium succinicans (Bacteroidetes)</th>
      <th>Prevotella copri (Bacteroidetes)</th>
      <th>Prevotella melaninogenica (Bacteroidetes)</th>
      <th>Prevotella stercorea (Bacteroidetes)</th>
      <th>Propionibacterium acnes (Actinobacteria)</th>
      <th>Pseudomonas veronii (Proteobacteria)</th>
      <th>Pseudomonas viridiflava (Proteobacteria)</th>
    </tr>
    <tr>
      <th>Predicted taxonomy</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Flavobacterium succinicans (Bacteroidetes)</th>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Prevotella copri (Bacteroidetes)</th>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Prevotella melaninogenica (Bacteroidetes)</th>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Prevotella stercorea (Bacteroidetes)</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Propionibacterium acnes (Actinobacteria)</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Pseudomonas veronii (Proteobacteria)</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Pseudomonas viridiflava (Proteobacteria)</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If the classifier generated here got some classifications wrong, did the classifier at least assign the sequences to the correct phylum?</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Jump back up to where we <a class="reference internal" href="#ml-define-k"><span class="std std-ref">defined <code class="docutils literal notranslate"><span class="pre">k</span></code> and <code class="docutils literal notranslate"><span class="pre">taxonomic_level</span></code></span></a> and modify those values. How does the accuracy of the classifier change if you increase or decrease <code class="docutils literal notranslate"><span class="pre">k</span></code> while keeping the value of <code class="docutils literal notranslate"><span class="pre">taxonomic_level</span></code> fixed? How does the accuracy change if you increase or decrease the <code class="docutils literal notranslate"><span class="pre">taxonomic_level</span></code> while keeping <code class="docutils literal notranslate"><span class="pre">k</span></code> fixed?</p>
</div>
</div>
</div>
<div class="section" id="variations-on-the-input-to-machine-learning-algorithms">
<h2>Variations on the input to machine learning algorithms<a class="headerlink" href="#variations-on-the-input-to-machine-learning-algorithms" title="Permalink to this headline">¶</a></h2>
<p>As in the Iris dataset, the labels in our microbial data are discrete (i.e., categorical or qualitative) as opposed to continuous (i.e., quantitative). If our labels in a supervised learning project were continous instead of discrete - for example the abundance of an organism in an environment - we could still apply supervised learning, but we would work with different algorithms. Specifically, we’d used supervised regression algorithms, rather than supervised classification algorithms.</p>
<p>Similarly, while the features we worked with in our unsupervised and supervised learning examples were continuous values, feature values could also be discrete (e.g., the sex of a subject, or the species of a specimen in an environment). The applicable algorithms might change, but machine learning techniques in general would still be available.</p>
<p>scikit-learn provides other example datasets, including <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes">the diabetes dataset</a>, <a class="reference external" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-house-prices-dataset">the housing market dataset</a> and <a class="reference external" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset">the hand-writing dataset</a>. These are good illustrations of other types of data that can be used in machine learning tasks. The message to take away is that if you can wrangle your data into a feature table, potentially with corresponding sample labels, you will likely be able to apply machine learning techniques to that data. That said, and as I mentioned at the beginning of this chapter, this introduction barely scratches the surface of this complex branch of statistics and computer science. Especially with the accessibility of these methods through software like scikit-learn, it’s easy to get to the point where you know enough to get yourself into trouble by using machine learning methods inappropriately. If you’d like to apply these tools in your research, you must continue your learning. I recommend continuing with <a class="reference external" href="https://scikit-learn.org/">scikit-learn’s documentation</a>.</p>
</div>
<div class="section" id="list-of-works-cited">
<h2>List of works cited<a class="headerlink" href="#list-of-works-cited" title="Permalink to this headline">¶</a></h2>
<p id="id3"><dl class="citation">
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id1">Fis36</a></span></dt>
<dd><p>R A Fisher. The use of multiple measurements in taxonomic problems. <em>Ann. Eugen.</em>, 7(2):179–188, September 1936.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id2">VazquezBPGK13</a></span></dt>
<dd><p>Yoshiki Vázquez-Baeza, Meg Pirrung, Antonio Gonzalez, and Rob Knight. EMPeror: a tool for visualizing high-throughput microbial community data. <em>Gigascience</em>, 2(1):16, November 2013.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="database-searching.html" title="previous page">Sequence homology searching</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By J Gregory Caporaso<br/>
        
            &copy; Copyright 2014-2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>